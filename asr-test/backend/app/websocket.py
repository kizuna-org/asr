import json
import asyncio
from typing import List, Dict, Optional
from fastapi import APIRouter, WebSocket, WebSocketDisconnect
import logging
import torch
import torchaudio
from . import config_loader
import traceback

router = APIRouter()
# WebSocketå°‚ç”¨ã®ãƒ­ã‚¬ãƒ¼
logger = logging.getLogger("websocket")

class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        try:
            self.loop = asyncio.get_running_loop()
        except RuntimeError:
            self.loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.loop)

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: Dict):
        """æ¥ç¶šã—ã¦ã„ã‚‹å…¨ã¦ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã™ã‚‹"""
        for connection in self.active_connections:
            await connection.send_text(json.dumps(message))

    def broadcast_sync(self, message: Dict):
        """åŒæœŸé–¢æ•°ã‹ã‚‰ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
        try:
            if self.loop.is_running():
                asyncio.run_coroutine_threadsafe(self.broadcast(message), self.loop)
            else:
                self.loop.run_until_complete(self.broadcast(message))
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¶™ç¶šã™ã‚‹
            print(f"WebSocket broadcast error: {e}")
            pass

# ConnectionManagerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
manager = ConnectionManager()

@router.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocketæ¥ç¶šã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    æ‹¡å¼µ: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®åˆ¶å¾¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸(JSON)ã¨éŸ³å£°ãƒã‚¤ãƒŠãƒª(PCM/float)ã‚’å—ä¿¡ã—ã€
    ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã—ã¦ä¸€å®šé–“éš”ã§éƒ¨åˆ†çš„ãªæ–‡å­—èµ·ã“ã—çµæœã‚’è¿”ã™ã€‚
    ãƒ—ãƒ­ãƒˆã‚³ãƒ«(æš«å®š):
      - Text(JSON): {"type":"start","model_name":"conformer","sample_rate":48000,"format":"f32"}
      - Text(JSON): {"type":"stop"}
      - Binary: éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ (PCM16LE ã‚‚ã—ãã¯ float32 ãƒªãƒˆãƒ«ã‚¨ãƒ³ãƒ‡ã‚£ã‚¢ãƒ³)ã€‚
    ã‚µãƒ¼ãƒå´ã¯ 16kHz mono ã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ãƒãƒƒãƒ•ã‚¡ã«è“„ç©ã—ã€
    ç´„1ç§’æ¯ã«ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’èµ°ã‚‰ã›ã¦ partial ã‚’è¿”ã™ã€‚
    """
    client_info = getattr(websocket, "client", None)
    await manager.connect(websocket)
    
    logger.info("ğŸ”Œ WebSocket connection established", 
                extra={"extra_fields": {"component": "websocket", "action": "connect", 
                                      "client": str(client_info), "connection_count": len(manager.active_connections)}})

    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çŠ¶æ…‹
    model = None  # type: Optional[torch.nn.Module]
    input_sample_rate = 16000
    input_dtype = "i16"  # "i16" or "f32"
    resampler = None  # type: Optional[torchaudio.transforms.Resample]
    buffer = []  # list[torch.Tensor]
    last_infer_time = 0.0

    def get_model_for_inference(model_name: str):
        # api.py ã¨ã®å¾ªç’°ä¾å­˜ã‚’é¿ã‘ã‚‹ãŸã‚ã€ã“ã“ã§ç°¡æ˜“å®Ÿè£…
        from .state import _model_cache
        if model_name not in _model_cache:
            model_config = config_loader.get_model_config(model_name)
            if not model_config:
                logger.error("Model config not found: %s", model_name)
                print(f"[WS] ERROR: Model config not found: {model_name}")
                raise RuntimeError(f"Model '{model_name}' not found in config.")
            import importlib
            if model_name == "realtime":
                ModelClass = getattr(importlib.import_module(f".models.{model_name}", "app"), "RealtimeASRModel")
            else:
                class_name = f"{model_name.capitalize()}ASRModel"
                ModelClass = getattr(importlib.import_module(f".models.{model_name}", "app"), class_name)
            class_name = ModelClass.__name__
            logger.info("Loading model class: %s", class_name)
            print(f"[WS] Loading model class: {class_name}")
            m = ModelClass(model_config)
            m.eval()
            _model_cache[model_name] = m
            print(f"[WS] Model {model_name} loaded and cached successfully")
        else:
            print(f"[WS] Using cached model: {model_name}")
        return _model_cache[model_name]

    try:
        while True:
            message = await websocket.receive()
            logger.debug("WS received message type=%s keys=%s", message.get("type"), list(message.keys()))
            mtype = message.get("type")

            if mtype == "websocket.disconnect":
                break

            if mtype == "websocket.receive":
                if "text" in message and message["text"] is not None:
                    try:
                        data = json.loads(message["text"])
                    except Exception:
                        logger.exception("Invalid JSON from client")
                        await websocket.send_text(json.dumps({"type": "error", "payload": {"message": "Invalid JSON"}}))
                        continue

                    if data.get("type") == "start":
                        model_name = data.get("model_name", "conformer")
                        input_sample_rate = int(data.get("sample_rate", 16000))
                        input_dtype = data.get("format", "i16")
                        if input_dtype not in ("i16", "f32"):
                            input_dtype = "i16"
                        
                        # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒªã‚µãƒ³ãƒ—ãƒ©åˆæœŸåŒ–
                        logger.info("ğŸš€ Starting audio streaming", 
                                   extra={"extra_fields": {"component": "websocket", "action": "start_streaming", 
                                                         "model_name": model_name, "sample_rate": input_sample_rate, 
                                                         "format": input_dtype, "client": str(client_info)}})
                        
                        model = get_model_for_inference(model_name)
                        if input_sample_rate != 16000:
                            resampler = torchaudio.transforms.Resample(orig_freq=input_sample_rate, new_freq=16000)
                            logger.info("Resampler initialized", 
                                       extra={"extra_fields": {"component": "websocket", "action": "resampler_init", 
                                                             "from_rate": input_sample_rate, "to_rate": 16000}})
                        else:
                            resampler = None
                        
                        buffer.clear()
                        last_infer_time = asyncio.get_event_loop().time()
                        await websocket.send_text(json.dumps({"type": "status", "payload": {"status": "ready"}}))
                        continue

                    if data.get("type") == "stop":
                        # æœ€çµ‚æ¨è«–ã‚’å®Ÿè¡Œ
                        if model is not None and len(buffer) > 0:
                            waveform = torch.cat(buffer) if len(buffer) > 1 else buffer[0]
                            try:
                                transcription = model.inference(waveform)
                                await websocket.send_text(json.dumps({"type": "final", "payload": {"text": transcription}}))
                            except Exception as e:
                                logger.exception("Final inference error")
                                await websocket.send_text(json.dumps({"type": "error", "payload": {"message": str(e)}}))
                        buffer.clear()
                        model = None
                        logger.info("ğŸ›‘ Stop streaming and cleared state", 
                                   extra={"extra_fields": {"component": "websocket", "action": "stop_streaming", 
                                                         "client": str(client_info)}})
                        print("[WS] stop streaming")
                        await websocket.send_text(json.dumps({"type": "status", "payload": {"status": "stopped"}}))
                        continue

                if "bytes" in message and message["bytes"] is not None:
                    # éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ å—ä¿¡
                    if model is None:
                        # start å‰ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¯ç„¡è¦–
                        logger.debug("Audio frame ignored because streaming not started", 
                                   extra={"extra_fields": {"component": "websocket", "action": "frame_ignored", 
                                                         "reason": "streaming_not_started"}})
                        continue
                    
                    raw = message["bytes"]
                    if not raw:
                        logger.debug("Empty audio chunk received", 
                                   extra={"extra_fields": {"component": "websocket", "action": "empty_chunk"}})
                        continue
                    
                    # éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ å—ä¿¡ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
                    # ãƒã‚¤ãƒŠãƒªã‚’ãƒ†ãƒ³ã‚½ãƒ«åŒ–
                    if input_dtype == "i16":
                        tensor = torch.frombuffer(raw, dtype=torch.int16).to(torch.float32) / 32768.0
                    elif input_dtype == "f32":
                        tensor = torch.frombuffer(raw, dtype=torch.float32)
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæœªæŒ‡å®šæ™‚ã¯è‡ªå‹•åˆ¤åˆ¥: 2ãƒã‚¤ãƒˆå˜ä½ãªã‚‰i16ã€4ãƒã‚¤ãƒˆå˜ä½ãªã‚‰f32
                        if len(raw) % 4 == 0:
                            tensor = torch.frombuffer(raw, dtype=torch.float32)
                            input_dtype = "f32"
                        else:
                            tensor = torch.frombuffer(raw, dtype=torch.int16).to(torch.float32) / 32768.0
                            input_dtype = "i16"

                    # mono å‰æã€‚å¿…è¦ãªã‚‰ãƒãƒ£ãƒãƒ«åˆ†é›¢ã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã§å®Ÿæ–½ã€‚
                    if resampler is not None:
                        tensor = resampler(tensor.unsqueeze(0)).squeeze(0)

                    buffer.append(tensor)
                    total_samples = sum(t.numel() for t in buffer)
                    total_duration_sec = total_samples / 16000  # 16kHzå‰æ
                    
                    # ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°çŠ¶æ³ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰

                    # ä¸€å®šé–“éš”ã§éƒ¨åˆ†æ¨è«–
                    now = asyncio.get_event_loop().time()
                    if now - last_infer_time >= 1.0 and len(buffer) > 0:
                        last_infer_time = now
                        try:
                            waveform = torch.cat(buffer) if len(buffer) > 1 else buffer[0]
                            
                            # éƒ¨åˆ†æ¨è«–é–‹å§‹ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
                            
                            import time
                            start_time = time.time()
                            transcription = model.inference(waveform)
                            inference_time = time.time() - start_time
                            
                            # éƒ¨åˆ†æ¨è«–å®Œäº†ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
                            
                            if transcription and transcription.strip():  # ç©ºã§ãªã„å ´åˆã®ã¿é€ä¿¡
                                await websocket.send_text(json.dumps({"type": "partial", "payload": {"text": transcription}}))
                                # éƒ¨åˆ†çµæœé€ä¿¡ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
                            else:
                                # ç©ºã®æ–‡å­—èµ·ã“ã—ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
                                pass
                        except Exception as e:
                            logger.error("Partial inference error", 
                                       extra={"extra_fields": {"component": "websocket", "action": "partial_inference_error", 
                                                             "error": str(e), "traceback": traceback.format_exc()}})
                            await websocket.send_text(json.dumps({"type": "error", "payload": {"message": str(e)}}))
                        # ãƒãƒƒãƒ•ã‚¡ã¯ç¶™ç¶šçš„ã«è“„ç©ï¼ˆå…¨ä½“ã§ã®æœ€è‰¯ä»®èª¬å‰æï¼‰ã€‚å¿…è¦ã«å¿œã˜ã¦æœ«å°¾æ•°ç§’ã«åˆ‡ã‚Šè©°ã‚å¯èƒ½ã€‚
                        # ä¾‹: æœ€æ–° 20 ç§’ã«åˆ¶é™
                        max_seconds = 20
                        max_samples = 16000 * max_seconds
                        total = sum(t.numel() for t in buffer)
                        if total > max_samples:
                            # å…ˆé ­ã‹ã‚‰å‰Šã‚‹
                            remain = total - max_samples
                            new_buf = []
                            for t in buffer:
                                if remain <= 0:
                                    new_buf.append(t)
                                    continue
                                if t.numel() <= remain:
                                    remain -= t.numel()
                                    continue
                                new_buf.append(t[remain:])
                                remain = 0
                            buffer = new_buf
                            logger.debug("Trimmed buffer to last %d seconds", max_seconds)
                        continue
    except WebSocketDisconnect:
        logger.info("ğŸ”Œ WebSocket disconnected", 
                   extra={"extra_fields": {"component": "websocket", "action": "disconnect", 
                                         "client": str(getattr(websocket, "client", None))}})
    except Exception as e:
        logger.error("WebSocket error", 
                   extra={"extra_fields": {"component": "websocket", "action": "error", 
                                         "error": str(e), "traceback": traceback.format_exc()}})
    finally:
        manager.disconnect(websocket)
        logger.info("ğŸ§¹ WebSocket cleanup completed", 
                   extra={"extra_fields": {"component": "websocket", "action": "cleanup"}})
