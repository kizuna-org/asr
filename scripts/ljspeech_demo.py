#!/usr/bin/env python3
"""
LJSpeech Dataset Learning Script (Speech Synthesis Version with Epoch Callback)
This script demonstrates how to load, work with, and train a Text-to-Speech model on the LJSpeech dataset.
This version has been modified to generate audio from text and includes a callback to output
a sample audio file at the end of each training epoch.
Enhanced with robust checkpoint and resume functionality.
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import librosa
import librosa.display
import os
import soundfile as sf
import json
import signal
import sys
import time
from datetime import datetime

# Set memory growth to avoid GPU memory issues
try:
    physical_devices = tf.config.list_physical_devices("GPU")
    if physical_devices:
        for device in physical_devices:
            tf.config.experimental.set_memory_growth(device, True)
        print(f"Found {len(physical_devices)} GPU(s)")
    else:
        print("No GPU devices found, using CPU")
except Exception as e:
    print(f"Warning: Could not configure GPU memory growth: {e}")

# Set the maximum number of frames for mel-spectrogram (for ~5 seconds audio)
MAX_FRAMES = 430  # (5 * 22050 / 256 â‰ˆ 430)

# Checkpoint paths
CHECKPOINT_DIR = "outputs/checkpoints"
MODEL_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, "model.keras")
TEXT_ENCODER_PATH = os.path.join(CHECKPOINT_DIR, "text_encoder")
TRAINING_STATE_PATH = os.path.join(CHECKPOINT_DIR, "training_state.json")
DATASET_CACHE_PATH = os.path.join(CHECKPOINT_DIR, "dataset_processed.cache")

# Global variables for graceful shutdown
training_interrupted = False
current_model = None
current_text_encoder = None
current_epoch = 0

def signal_handler(signum, frame):
    """Handle interrupt signals (Ctrl+C) gracefully."""
    global training_interrupted, current_model, current_text_encoder, current_epoch
    print(f"\n\n=== ä¸­æ–­ã‚·ã‚°ãƒŠãƒ«ã‚’å—ä¿¡ã—ã¾ã—ãŸ (Signal: {signum}) ===")
    print("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®‰å…¨ã«åœæ­¢ã—ã¦ã„ã¾ã™...")
    training_interrupted = True
    
    if current_model is not None and current_text_encoder is not None:
        print("ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜ä¸­...")
        try:
            save_training_state(current_epoch, current_text_encoder, current_model)
            print("âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    print("ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™...")
    sys.exit(0)

# Register signal handlers
signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
signal.signal(signal.SIGTERM, signal_handler)  # Termination signal

def load_ljspeech_dataset(
    split: str = "train", batch_size: int = 32
) -> tf.data.Dataset:
    """
    Load the LJSpeech dataset from TensorFlow Datasets.
    """
    print(f"Loading LJSpeech dataset with split: {split}")

    try:
        import tensorflow_datasets as tfds
    except ImportError as e:
        print(f"Error importing tensorflow_datasets: {e}")
        print("Please install tensorflow_datasets: pip install tensorflow-datasets")
        raise

    dataset, info = tfds.load(
        "ljspeech",
        split=split,
        with_info=True,
        as_supervised=True,
        data_dir="/opt/datasets",
    )

    print(f"Dataset info: {info}")
    print(f"Number of examples: {info.splits[split].num_examples}")

    return dataset.batch(batch_size)


def preprocess_audio(audio: tf.Tensor) -> tf.Tensor:
    """
    Preprocess audio data for machine learning.
    """
    audio = tf.cast(audio, tf.float32) / 32768.0
    if len(audio.shape) > 1 and audio.shape[-1] > 1:
        audio = tf.reduce_mean(audio, axis=-1)
    return audio


def extract_mel_spectrogram(
    audio: tf.Tensor,
    sample_rate: int = 22050,
    n_mels: int = 80,
    n_fft: int = 1024,
    hop_length: int = 256,
) -> tf.Tensor:
    """
    Extract mel-spectrogram features from audio.
    """
    audio_np = audio.numpy()
    mel_spec = librosa.feature.melspectrogram(
        y=audio_np, sr=sample_rate, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length
    )
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_spec_db = mel_spec_db.T  # Transpose to [time, n_mels]
    return tf.convert_to_tensor(mel_spec_db, dtype=tf.float32)


def create_text_encoder(vocab_size: int = 1000) -> tf.keras.layers.TextVectorization:
    """
    Create a text encoder for processing transcriptions.
    """
    return tf.keras.layers.TextVectorization(
        max_tokens=vocab_size,
        output_sequence_length=MAX_FRAMES,  # Use MAX_FRAMES for output sequence length
        standardize="lower_and_strip_punctuation",
    )


def build_text_to_spectrogram_model(
    vocab_size: int, mel_bins: int = 80, max_sequence_length: int = MAX_FRAMES
) -> tf.keras.Model:
    """
    Build a simple Text-to-Spectrogram model for speech synthesis.
    """
    text_input = tf.keras.layers.Input(shape=(None,), name="text_input")

    # Text Embedding and Encoding
    text_features = tf.keras.layers.Embedding(vocab_size, 2, mask_zero=True)(
        text_input
    )
    text_features = tf.keras.layers.LSTM(4, return_sequences=True)(text_features)
    text_features = tf.keras.layers.LSTM(4, return_sequences=True)(text_features)

    # Output layer to predict mel-spectrogram
    mel_output = tf.keras.layers.TimeDistributed(
        tf.keras.layers.Dense(mel_bins, activation="linear")
    )(text_features)

    model = tf.keras.Model(inputs=text_input, outputs=mel_output)

    # Use Mean Squared Error to measure the difference between predicted and actual spectrograms
    model.compile(optimizer="adam", loss="mean_squared_error", metrics=["mae"])
    return model


def visualize_audio_and_spectrogram(
    audio: tf.Tensor, text: str, sample_rate: int = 22050, save_path: str | None = None
):
    """
    Visualize audio waveform and mel-spectrogram.
    """
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    audio_np = audio.numpy()
    time = np.arange(len(audio_np)) / sample_rate
    ax1.plot(time, audio_np)
    ax1.set_title(f'Waveform - Text: "{text[:50]}..."')
    ax1.set_xlabel("Time (s)")
    ax1.set_ylabel("Amplitude")
    ax1.grid(True)

    mel_spec_db = extract_mel_spectrogram(audio)
    mel_spec_db_np = mel_spec_db.numpy()
    librosa.display.specshow(
        mel_spec_db_np.T,  # Transpose back for display
        sr=sample_rate,
        hop_length=256,
        x_axis="time",
        y_axis="mel",
        ax=ax2,
    )
    ax2.set_title("Mel-Spectrogram")
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"Plot saved to {save_path}")
    plt.show()


class SynthesisCallback(tf.keras.callbacks.Callback):
    def __init__(self, text_encoder, n_fft, hop_length, sample_rate=22050):
        super().__init__()
        self.text_encoder = text_encoder
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.sample_rate = sample_rate
        # éŸ³å£°åˆæˆã«ä½¿ç”¨ã™ã‚‹ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆ
        self.inference_text = "This is a test of the model at the end of each epoch."
        os.makedirs("outputs/epoch_samples", exist_ok=True)
        
        # æ™‚é–“äºˆæ¸¬ç”¨ã®å¤‰æ•°
        self.training_start_time = None
        self.epoch_start_time = None
        self.epoch_times = []  # å„ã‚¨ãƒãƒƒã‚¯ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨˜éŒ²
        self.total_epochs = None

    def on_train_begin(self, logs=None):
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹æ™‚ã«é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²ã—ã€ç·ã‚¨ãƒãƒƒã‚¯æ•°ã‚’è¨­å®š"""
        self.training_start_time = time.time()
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ç·ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å–å¾—
        self.total_epochs = self.params.get('epochs', 3)
        start_time_str = datetime.fromtimestamp(self.training_start_time).strftime('%Y-%m-%d %H:%M:%S')
        print(f"\nğŸš€ å­¦ç¿’é–‹å§‹æ™‚åˆ»: {start_time_str}")
        print(f"ğŸ“Š ç·ã‚¨ãƒãƒƒã‚¯æ•°: {self.total_epochs}")

    def on_epoch_begin(self, epoch, logs=None):
        """Update global variables at the start of each epoch."""
        global current_epoch, current_model, current_text_encoder
        current_epoch = epoch
        current_model = self.model
        current_text_encoder = self.text_encoder
        
        # ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
        self.epoch_start_time = time.time()
        epoch_start_str = datetime.fromtimestamp(self.epoch_start_time).strftime('%H:%M:%S')
        print(f"\nğŸš€ ã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{self.total_epochs} ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™... (é–‹å§‹æ™‚åˆ»: {epoch_start_str})")

    def on_epoch_end(self, epoch, logs=None):
        """Generate audio sample, save state, and predict completion time at the end of each epoch."""
        global training_interrupted
        
        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²
        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - self.epoch_start_time
        self.epoch_times.append(epoch_duration)
        
        # æ™‚é–“äºˆæ¸¬ã®è¨ˆç®—
        completed_epochs = epoch + 1
        remaining_epochs = self.total_epochs - completed_epochs
        
        # å¹³å‡ã‚¨ãƒãƒƒã‚¯æ™‚é–“ã‚’è¨ˆç®—
        avg_epoch_time = sum(self.epoch_times) / len(self.epoch_times)
        
        # æ®‹ã‚Šæ™‚é–“ã‚’è¨ˆç®—
        estimated_remaining_time = remaining_epochs * avg_epoch_time
        estimated_completion_time = epoch_end_time + estimated_remaining_time
        
        # æ™‚é–“æƒ…å ±ã‚’è¡¨ç¤º
        epoch_duration_min = epoch_duration / 60
        avg_epoch_time_min = avg_epoch_time / 60
        remaining_time_min = estimated_remaining_time / 60
        
        completion_time_str = datetime.fromtimestamp(estimated_completion_time).strftime('%Y-%m-%d %H:%M:%S')
        
        print(f"\nâ±ï¸  === ã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{self.total_epochs} æ™‚é–“æƒ…å ± ===")
        print(f"ğŸ“ˆ ä»Šå›ã®ã‚¨ãƒãƒƒã‚¯å®Ÿè¡Œæ™‚é–“: {epoch_duration_min:.1f}åˆ†")
        print(f"ğŸ“Š å¹³å‡ã‚¨ãƒãƒƒã‚¯å®Ÿè¡Œæ™‚é–“: {avg_epoch_time_min:.1f}åˆ†")
        
        if remaining_epochs > 0:
            print(f"â³ æ®‹ã‚Šã‚¨ãƒãƒƒã‚¯æ•°: {remaining_epochs}")
            print(f"ğŸ• æ¨å®šæ®‹ã‚Šæ™‚é–“: {remaining_time_min:.1f}åˆ†")
            print(f"ğŸ æ¨å®šå®Œäº†æ™‚åˆ»: {completion_time_str}")
        else:
            total_training_time = (epoch_end_time - self.training_start_time) / 60
            print(f"ğŸ‰ å…¨å­¦ç¿’å®Œäº†ï¼ç·å­¦ç¿’æ™‚é–“: {total_training_time:.1f}åˆ†")
        
        print("=" * 50)
        
        if training_interrupted:
            print("\nâš ï¸  ä¸­æ–­ãŒè¦æ±‚ã•ã‚Œã¾ã—ãŸã€‚éŸ³å£°ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return
            
        print(f"\n\n--- ã‚¨ãƒãƒƒã‚¯ {epoch + 1} çµ‚äº†æ™‚ã®éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ ---")

        try:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            text_vec = self.text_encoder([self.inference_text])

            # ãƒ¢ãƒ‡ãƒ«ã§ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’äºˆæ¸¬
            predicted_mel_spec = self.model.predict(text_vec)

            # ãƒãƒƒãƒæ¬¡å…ƒã‚’å‰Šé™¤ã—ã€numpyé…åˆ—ã«å¤‰æ›
            predicted_mel_spec_np = predicted_mel_spec[0]

            # ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’ãƒ‡ã‚·ãƒ™ãƒ«ã‹ã‚‰ãƒ‘ãƒ¯ãƒ¼ã«å¤‰æ›
            predicted_mel_spec_db_t = predicted_mel_spec_np.T
            power_spec = librosa.db_to_power(predicted_mel_spec_db_t)

            # Griffin-Limã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§éŸ³å£°ã‚’å¾©å…ƒ
            generated_audio = librosa.feature.inverse.mel_to_audio(
                power_spec,
                sr=self.sample_rate,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
            )

            # ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ã‚’ä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_audio_path = f"outputs/epoch_samples/epoch_{epoch + 1}_{timestamp}.wav"
            sf.write(output_audio_path, generated_audio, self.sample_rate)
            print(f"ğŸµ ã‚¨ãƒãƒƒã‚¯ {epoch + 1} ã®éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜: {output_audio_path}")

        except Exception as e:
            print(f"âŒ ã‚¨ãƒãƒƒã‚¯ {epoch + 1} ã®éŸ³å£°ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()

    def on_train_end(self, logs=None):
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµ‚äº†æ™‚ã®æœ€çµ‚æƒ…å ±ã‚’è¡¨ç¤º"""
        if self.training_start_time is not None:
            total_training_time = (time.time() - self.training_start_time) / 60
            end_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            print(f"\nğŸ‰ === å­¦ç¿’å®Œäº†æƒ…å ± ===")
            print(f"ğŸ å­¦ç¿’çµ‚äº†æ™‚åˆ»: {end_time_str}")
            print(f"â±ï¸  ç·å­¦ç¿’æ™‚é–“: {total_training_time:.1f}åˆ†")
            print(f"ğŸ“Š å®Ÿè¡Œã•ã‚ŒãŸã‚¨ãƒãƒƒã‚¯æ•°: {len(self.epoch_times)}")
            if self.epoch_times:
                avg_time = sum(self.epoch_times) / len(self.epoch_times) / 60
                print(f"ğŸ“ˆ å¹³å‡ã‚¨ãƒãƒƒã‚¯æ™‚é–“: {avg_time:.1f}åˆ†")
            print("=" * 50)


def save_training_state(epoch, text_encoder, model):
    """Save training state including epoch number and text encoder."""
    try:
        os.makedirs(CHECKPOINT_DIR, exist_ok=True)

        print(f"ğŸ’¾ ã‚¨ãƒãƒƒã‚¯ {epoch} ã®çŠ¶æ…‹ã‚’ä¿å­˜ä¸­...")
        
        # Save model
        model.save(MODEL_CHECKPOINT_PATH)
        print(f"  âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {MODEL_CHECKPOINT_PATH}")

        # Save text encoder vocabulary
        vocab = text_encoder.get_vocabulary()
        vocab_path = os.path.join(CHECKPOINT_DIR, "vocabulary.json")
        with open(vocab_path, "w", encoding='utf-8') as f:
            json.dump(vocab, f, ensure_ascii=False, indent=2)
        print(f"  âœ… èªå½™ã‚’ä¿å­˜: {vocab_path}")

        # Save training state with timestamp
        training_state = {
            "epoch": epoch,
            "vocab_size": text_encoder.vocabulary_size(),
            "max_tokens": text_encoder._max_tokens,
            "output_sequence_length": text_encoder._output_sequence_length,
            "standardize": text_encoder._standardize,
            "saved_at": datetime.now().isoformat(),
            "tensorflow_version": tf.__version__,
            "checkpoint_version": "2.0"
        }
        
        # Create backup of previous state
        if os.path.exists(TRAINING_STATE_PATH):
            backup_path = TRAINING_STATE_PATH + ".backup"
            os.rename(TRAINING_STATE_PATH, backup_path)
            print(f"  ğŸ“‹ å‰å›ã®çŠ¶æ…‹ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path}")
        
        with open(TRAINING_STATE_PATH, "w", encoding='utf-8') as f:
            json.dump(training_state, f, ensure_ascii=False, indent=2)
        print(f"  âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’ä¿å­˜: {TRAINING_STATE_PATH}")

        print(f"ğŸ’¾ ã‚¨ãƒãƒƒã‚¯ {epoch} ã®ä¿å­˜å®Œäº†")
        
    except Exception as e:
        print(f"âŒ çŠ¶æ…‹ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        raise


def load_training_state():
    """Load training state and return epoch number, text encoder, and model if available."""
    print("ğŸ” ä¿å­˜ã•ã‚ŒãŸçŠ¶æ…‹ã‚’ç¢ºèªä¸­...")
    
    if not os.path.exists(TRAINING_STATE_PATH):
        print("â„¹ï¸  ä¿å­˜ã•ã‚ŒãŸçŠ¶æ…‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æœ€åˆã‹ã‚‰é–‹å§‹ã—ã¾ã™ã€‚")
        return 0, None, None

    try:
        # Load training state
        print(f"ğŸ“‚ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿ä¸­: {TRAINING_STATE_PATH}")
        with open(TRAINING_STATE_PATH, "r", encoding='utf-8') as f:
            training_state = json.load(f)
        
        # Validate checkpoint version compatibility
        checkpoint_version = training_state.get("checkpoint_version", "1.0")
        print(f"  ğŸ“‹ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³: {checkpoint_version}")
        
        if training_state.get("tensorflow_version"):
            print(f"  ğŸ”§ ä¿å­˜æ™‚ã®TensorFlowãƒãƒ¼ã‚¸ãƒ§ãƒ³: {training_state['tensorflow_version']}")
            print(f"  ğŸ”§ ç¾åœ¨ã®TensorFlowãƒãƒ¼ã‚¸ãƒ§ãƒ³: {tf.__version__}")
        
        saved_at = training_state.get("saved_at", "ä¸æ˜")
        print(f"  â° ä¿å­˜æ™‚åˆ»: {saved_at}")

        # Check if model file exists
        if not os.path.exists(MODEL_CHECKPOINT_PATH):
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MODEL_CHECKPOINT_PATH}")
            return 0, None, None

        # Load vocabulary
        vocab_path = os.path.join(CHECKPOINT_DIR, "vocabulary.json")
        if not os.path.exists(vocab_path):
            print(f"âŒ èªå½™ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {vocab_path}")
            return 0, None, None
            
        print(f"ğŸ“– èªå½™ã‚’èª­ã¿è¾¼ã¿ä¸­: {vocab_path}")
        with open(vocab_path, "r", encoding='utf-8') as f:
            vocab = json.load(f)

        # Recreate text encoder
        print("ğŸ”¤ ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’å†æ§‹ç¯‰ä¸­...")
        text_encoder = tf.keras.layers.TextVectorization(
            max_tokens=training_state["max_tokens"],
            output_sequence_length=training_state["output_sequence_length"],
            standardize=training_state["standardize"],
        )
        text_encoder.set_vocabulary(vocab)
        print(f"  âœ… èªå½™ã‚µã‚¤ã‚º: {len(vocab)}")

        # Load model
        print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {MODEL_CHECKPOINT_PATH}")
        model = tf.keras.models.load_model(MODEL_CHECKPOINT_PATH)
        print(f"  âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

        epoch = training_state["epoch"]
        print(f"ğŸ¯ ã‚¨ãƒãƒƒã‚¯ {epoch} ã‹ã‚‰å†é–‹ã—ã¾ã™")
        print("=" * 50)
        
        return epoch, text_encoder, model

    except Exception as e:
        print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("âš ï¸  æœ€åˆã‹ã‚‰é–‹å§‹ã—ã¾ã™...")
        import traceback
        traceback.print_exc()
        
        # Try to use backup if available
        backup_path = TRAINING_STATE_PATH + ".backup"
        if os.path.exists(backup_path):
            print(f"ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒã‚’è©¦è¡Œä¸­: {backup_path}")
            try:
                os.rename(backup_path, TRAINING_STATE_PATH)
                return load_training_state()  # Recursive call with backup
            except Exception as backup_error:
                print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©å…ƒã‚‚å¤±æ•—ã—ã¾ã—ãŸ: {backup_error}")
        
        return 0, None, None


def main():
    """Main function to run the LJSpeech learning script."""
    print("=== LJSpeech éŸ³å£°åˆæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ ===")
    print(f"ğŸ• é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)
    
    try:
        # Check for existing checkpoint
        start_epoch, text_encoder, model = load_training_state()

        if start_epoch > 0:
            print(f"ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ã‚¨ãƒãƒƒã‚¯ {start_epoch + 1} ã‹ã‚‰å†é–‹ã—ã¾ã™")
        else:
            print("ğŸ†• æ–°è¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™")

        # Load dataset
        print("\n=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ ===")
        dataset = load_ljspeech_dataset(split="train", batch_size=1)
        os.makedirs("outputs", exist_ok=True)

        # Print dataset size before preparation
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’ç¢ºèªä¸­...")
        num_before = (
            dataset.unbatch()
            .reduce(tf.constant(0, dtype=tf.int64), lambda x, _: x + 1)
            .numpy()
        )
        print(f"ğŸ“ˆ å‰å‡¦ç†å‰ã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {num_before:,}")

        # Text processing setup
        print("\n=== ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç† ===")
        if text_encoder is None:
            print("ğŸ”¤ æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆä¸­...")
            text_encoder = create_text_encoder(vocab_size=1000)
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§èªå½™ã‚’æ§‹ç¯‰
            print("ğŸ“š èªå½™ã‚’æ§‹ç¯‰ä¸­...")
            example_texts = dataset.unbatch().map(lambda text, audio: text).take(5000)
            text_encoder.adapt(example_texts)
        else:
            print("â™»ï¸  ä¿å­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨")
            
        print(f"ğŸ“– èªå½™ã‚µã‚¤ã‚º: {text_encoder.vocabulary_size():,}")

        # Build model
        print("\n=== ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ===")
        if model is None:
            print("ğŸ¤– æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ä¸­...")
            model = build_text_to_spectrogram_model(
                vocab_size=text_encoder.vocabulary_size(),
                mel_bins=80,
                max_sequence_length=MAX_FRAMES,
            )
        else:
            print("â™»ï¸  ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
            
        print("ğŸ“‹ ãƒ¢ãƒ‡ãƒ«æ§‹é€ :")
        model.summary()

        # --- Training Part ---
        print("\n=== ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ ===")

        # ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ è¨ˆç®—ã«å¿…è¦ãªæœ€å°ã®éŸ³å£°é•·ã‚’å®šç¾©
        N_FFT = 1024
        HOP_LENGTH = 256

        def filter_short_audio(text, audio):
            """
            éŸ³å£°ã®é•·ã•ãŒn_fftã‚ˆã‚ŠçŸ­ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é™¤å¤–ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°ã€‚
            """
            return tf.shape(audio)[0] > N_FFT

        def py_extract_mel_spectrogram_wrapper(audio):
            # ã“ã®é–¢æ•°ã¯tf.py_functionå†…ã§å‘¼ã°ã‚Œã‚‹
            return extract_mel_spectrogram(audio, n_fft=N_FFT, hop_length=HOP_LENGTH)

        def prepare_for_training(text, audio):
            """
            Prepare data for text-to-spectrogram training.
            Input: text vector
            Output: mel-spectrogram
            """
            audio_processed = preprocess_audio(audio)
            mel_spec = tf.py_function(
                func=py_extract_mel_spectrogram_wrapper,
                inp=[audio_processed],
                Tout=tf.float32,
            )
            mel_spec.set_shape((None, 80))  # Set shape for Keras
            text_vec = text_encoder(text)
            # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’MAX_FRAMESãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/åˆ‡ã‚Šè©°ã‚
            mel_spec = mel_spec[:MAX_FRAMES]  # åˆ‡ã‚Šè©°ã‚
            mel_spec = tf.pad(
                mel_spec, [[0, MAX_FRAMES - tf.shape(mel_spec)[0]], [0, 0]]
            )  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            mel_spec.set_shape((MAX_FRAMES, 80))
            return text_vec, mel_spec

        print("âš™ï¸  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ä¸­...")
        train_dataset = (
            dataset.unbatch()
            .filter(filter_short_audio)  # çŸ­ã™ãã‚‹éŸ³å£°ã‚’é™¤å¤–
            .map(prepare_for_training, num_parallel_calls=tf.data.AUTOTUNE)
            .cache()
            .shuffle(buffer_size=1024)
            .padded_batch(
                batch_size=32,
                padded_shapes=(
                    tf.TensorShape([None]),  # Shape for text_vec
                    tf.TensorShape([None, 80]),  # Shape for mel_spec
                ),
            )
            .prefetch(tf.data.AUTOTUNE)
        )

        # Print dataset size after preparation
        print("ğŸ“Š å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’ç¢ºèªä¸­...")
        num_after = 0
        for _ in train_dataset.unbatch():
            num_after += 1
        print(f"ğŸ“ˆ å‰å‡¦ç†å¾Œã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {num_after:,}")

        print("\n=== ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ ===")

        # ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«éŸ³å£°ã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½œæˆ
        synthesis_callback = SynthesisCallback(
            text_encoder=text_encoder, n_fft=N_FFT, hop_length=HOP_LENGTH
        )

        # Create checkpoint callback
        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=MODEL_CHECKPOINT_PATH,
            save_best_only=False,
            save_weights_only=False,
            save_freq="epoch",
        )

        # Create custom callback to save training state
        class TrainingStateCallback(tf.keras.callbacks.Callback):
            def __init__(self, text_encoder):
                super().__init__()
                self.text_encoder = text_encoder

            def on_epoch_end(self, epoch, logs=None):
                """Save training state at the end of each epoch."""
                global training_interrupted
                
                if training_interrupted:
                    print("\nâš ï¸  ä¸­æ–­ãŒè¦æ±‚ã•ã‚ŒãŸãŸã‚ã€çŠ¶æ…‹ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    return
                
                try:
                    save_training_state(epoch + 1, self.text_encoder, self.model)  # Save next epoch number
                except Exception as e:
                    print(f"âŒ çŠ¶æ…‹ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç¶™ç¶šã—ã¾ã™: {e}")

            def on_batch_end(self, batch, logs=None):
                """Check for interruption during training."""
                global training_interrupted
                if training_interrupted:
                    print("\nâš ï¸  ä¸­æ–­ãŒè¦æ±‚ã•ã‚Œã¾ã—ãŸã€‚ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ã‚’å®Œäº†å¾Œã«åœæ­¢ã—ã¾ã™ã€‚")
                    self.model.stop_training = True

        training_state_callback = TrainingStateCallback(text_encoder)

        # Update global variables before training
        global current_model, current_text_encoder, current_epoch
        current_model = model
        current_text_encoder = text_encoder
        current_epoch = start_epoch

        print(f"ğŸ¯ ã‚¨ãƒãƒƒã‚¯ {start_epoch + 1} ã‹ã‚‰ {3} ã¾ã§å­¦ç¿’ã—ã¾ã™")
        print("ğŸ’¡ Ctrl+C ã§å®‰å…¨ã«ä¸­æ–­ã§ãã¾ã™")
        print("=" * 50)
        
        # model.fitã«callbackså¼•æ•°ã‚’è¿½åŠ 
        history = model.fit(
            train_dataset,
            epochs=3,
            initial_epoch=start_epoch,
            callbacks=[
                synthesis_callback,
                checkpoint_callback,
                training_state_callback,
            ],
        )

        # Check if training was interrupted
        global training_interrupted
        if training_interrupted:
            print("\nâš ï¸  ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
            print("ğŸ’¾ æœ€çµ‚çŠ¶æ…‹ã‚’ä¿å­˜ä¸­...")
            try:
                save_training_state(current_epoch, text_encoder, model)
                print("âœ… ä¸­æ–­æ™‚ã®çŠ¶æ…‹ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ")
            except Exception as e:
                print(f"âŒ ä¸­æ–­æ™‚ã®çŠ¶æ…‹ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return  # Early return on interruption

        print("\n=== ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸ! ===")

        # --- Save the Trained Model ---
        print("\n=== æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜ ===")
        model_save_path = "outputs/ljspeech_synthesis_model.keras"
        model.save(model_save_path)
        print(f"ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {model_save_path}")

        # Save final training state
        save_training_state(3, text_encoder, model)  # Final epoch

        # --- Perform Final Inference (Text-to-Speech) ---
        print("\n=== æœ€çµ‚æ¨è«–å®Ÿè¡Œ (ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°) ===")

        # æ¨è«–ã«ä½¿ç”¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        inference_text = (
            "Hello, this is a final test of the new speech synthesis model."
        )
        print(f"ğŸ¤ åˆæˆç”¨ãƒ†ã‚­ã‚¹ãƒˆ: '{inference_text}'")

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        text_vec = text_encoder([inference_text])

        # ãƒ¢ãƒ‡ãƒ«ã§ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’äºˆæ¸¬
        predicted_mel_spec = model.predict(text_vec)

        # ãƒãƒƒãƒæ¬¡å…ƒã‚’å‰Šé™¤ã—ã€numpyé…åˆ—ã«å¤‰æ›
        predicted_mel_spec_np = predicted_mel_spec[0]

        # ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’ãƒ‡ã‚·ãƒ™ãƒ«ã‹ã‚‰ãƒ‘ãƒ¯ãƒ¼ã«å¤‰æ›
        predicted_mel_spec_db_t = predicted_mel_spec_np.T
        power_spec = librosa.db_to_power(predicted_mel_spec_db_t)

        # Griffin-Limã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§éŸ³å£°ã‚’å¾©å…ƒ
        print("ğŸµ Griffin-Limã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§éŸ³å£°ã‚’åˆæˆä¸­...")
        generated_audio = librosa.feature.inverse.mel_to_audio(
            power_spec, sr=22050, n_fft=N_FFT, hop_length=HOP_LENGTH
        )

        # ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ã‚’ä¿å­˜
        output_audio_path = "outputs/synthesized_audio_final.wav"
        sf.write(output_audio_path, generated_audio, 22050)
        print(f"ğŸµ æœ€çµ‚åˆæˆéŸ³å£°ã‚’ä¿å­˜: {output_audio_path}")

        # ç”Ÿæˆã•ã‚ŒãŸã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã¨éŸ³å£°ã‚’å¯è¦–åŒ–
        visualize_audio_and_spectrogram(
            tf.convert_to_tensor(generated_audio),
            inference_text,
            save_path="outputs/synthesis_visualization_final.png",
        )

        print(f"\nğŸ• å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("\nâœ… ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        signal_handler(signal.SIGINT, None)
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        
        # Try to save current state if possible
        try:
            if 'current_model' in globals() and current_model is not None:
                print("ğŸ†˜ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®ç·Šæ€¥çŠ¶æ…‹ä¿å­˜ã‚’è©¦è¡Œä¸­...")
                save_training_state(current_epoch, current_text_encoder, current_model)
                print("âœ… ç·Šæ€¥çŠ¶æ…‹ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ")
        except Exception as save_error:
            print(f"âŒ ç·Šæ€¥çŠ¶æ…‹ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {save_error}")


if __name__ == "__main__":
    main()
