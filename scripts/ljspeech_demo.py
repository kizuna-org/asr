#!/usr/bin/env python3
"""
LJSpeech Dataset Learning Script (Speech Synthesis Version with Model Selection)
This script demonstrates how to load, work with, and train Text-to-Speech models on the LJSpeech dataset.
Supports multiple TTS model architectures including FastSpeech 2.
Enhanced with robust checkpoint and resume functionality.

Model Selection Features:
- FastSpeech 2: Advanced non-autoregressive TTS model with variance prediction
- Transformer TTS: Simple transformer-based TTS model for comparison
- Model-specific loss functions and training configurations
- Automatic checkpoint compatibility checking

Usage Examples:
  # Train with FastSpeech 2 (default)
  python ljspeech_demo.py --mode mini --epochs 100 --model fastspeech2
  
  # Train with simple Transformer TTS
  python ljspeech_demo.py --mode mini --epochs 100 --model transformer_tts
  
  # Full dataset training with FastSpeech 2
  python ljspeech_demo.py --mode full --epochs 2000 --model fastspeech2
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import librosa
import librosa.display
import os
import soundfile as sf
import json
import signal
import sys
import time
from datetime import datetime
from enum import Enum
from typing import Dict, Any, Optional

# Set memory growth to avoid GPU memory issues
try:
    physical_devices = tf.config.list_physical_devices("GPU")
    if physical_devices:
        for device in physical_devices:
            tf.config.experimental.set_memory_growth(device, True)
        print(f"Found {len(physical_devices)} GPU(s)")
    else:
        print("No GPU devices found, using CPU")
except Exception as e:
    print(f"Warning: Could not configure GPU memory growth: {e}")

# Model selection enumeration
class TTSModel(Enum):
    FASTSPEECH2 = "fastspeech2"
    TACOTRON2 = "tacotron2"  # Future implementation
    TRANSFORMER_TTS = "transformer_tts"  # Now implemented
    VITS = "vits"  # Now implemented

# Set the maximum number of frames for mel-spectrogram (for ~5 seconds audio)
MAX_FRAMES = 430  # (5 * 22050 / 256 â‰ˆ 430)

# Base paths - will be updated based on model type
BASE_OUTPUT_DIR = "outputs"
CHECKPOINT_DIR = None
MODEL_CHECKPOINT_PATH = None
TEXT_ENCODER_PATH = None
TRAINING_STATE_PATH = None
DATASET_CACHE_PATH = None

def setup_model_paths(model_type: TTSModel, limit_samples: int = None, mode: str = "mini"):
    """Setup model-specific paths based on the selected model type, sample count, and mode."""
    global CHECKPOINT_DIR, MODEL_CHECKPOINT_PATH, TEXT_ENCODER_PATH, TRAINING_STATE_PATH, DATASET_CACHE_PATH
    
    # Create sample-specific directory name
    if limit_samples is not None:
        sample_dir = f"samples_{limit_samples}"
    elif mode == 'full':
        sample_dir = "full_dataset"
    else:  # mini mode
        sample_dir = "samples_10"
    
    # Create hierarchical directory: outputs/model_type/sample_dir
    model_output_dir = os.path.join(BASE_OUTPUT_DIR, model_type.value, sample_dir)
    CHECKPOINT_DIR = os.path.join(model_output_dir, "checkpoints")
    MODEL_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, "model.keras")
    TEXT_ENCODER_PATH = os.path.join(CHECKPOINT_DIR, "text_encoder")
    TRAINING_STATE_PATH = os.path.join(CHECKPOINT_DIR, "training_state.json")
    DATASET_CACHE_PATH = os.path.join(CHECKPOINT_DIR, "dataset_processed.cache")
    
    # Create directories if they don't exist
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(os.path.join(model_output_dir, "epoch_samples"), exist_ok=True)
    
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š: {model_output_dir}")
    print(f"ğŸ“ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {CHECKPOINT_DIR}")
    print(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«è¨­å®š: {sample_dir}")

    return model_output_dir

# Global variables for graceful shutdown
training_interrupted = False
current_model = None
current_text_encoder = None
current_epoch = 0

def signal_handler(signum, frame):
    """Handle interrupt signals (Ctrl+C) gracefully."""
    global training_interrupted, current_model, current_text_encoder, current_epoch
    print(f"\n\n=== ä¸­æ–­ã‚·ã‚°ãƒŠãƒ«ã‚’å—ä¿¡ã—ã¾ã—ãŸ (Signal: {signum}) ===")
    print("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®‰å…¨ã«åœæ­¢ã—ã¦ã„ã¾ã™...")
    training_interrupted = True
    
    if current_model is not None and current_text_encoder is not None:
        print("ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜ä¸­...")
        try:
            # Use default values for signal handler save
            save_training_state(current_epoch, current_text_encoder, current_model, TTSModel.FASTSPEECH2, 10, "mini")
            print("âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    print("ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™...")
    sys.exit(0)

# Register signal handlers
signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
signal.signal(signal.SIGTERM, signal_handler)  # Termination signal


class FastSpeech2Loss(tf.keras.losses.Loss):
    """Custom loss function for FastSpeech 2 model."""
    
    def __init__(self, mel_loss_weight=1.0, duration_loss_weight=1.0, 
                 pitch_loss_weight=1.0, energy_loss_weight=1.0, **kwargs):
        super().__init__(**kwargs)
        self.mel_loss_weight = mel_loss_weight
        self.duration_loss_weight = duration_loss_weight
        self.pitch_loss_weight = pitch_loss_weight
        self.energy_loss_weight = energy_loss_weight
        
        self.mse = tf.keras.losses.MeanSquaredError()
        self.mae = tf.keras.losses.MeanAbsoluteError()
    
    def call(self, y_true, y_pred):
        """
        Compute loss for FastSpeech 2 model.
        
        y_true: mel-spectrogram (batch_size, time_steps, n_mels)
        y_pred: dict containing model outputs
        """
        # Mel-spectrogram loss (both before and after postnet)
        mel_loss = self.mse(y_true, y_pred['mel_output'])
        mel_postnet_loss = self.mse(y_true, y_pred['mel_output_refined'])
        total_mel_loss = mel_loss + mel_postnet_loss
        
        # For duration, pitch, and energy, we use simplified target (zeros for now)
        # In a real implementation, these would be extracted from the data
        batch_size = tf.shape(y_true)[0]
        seq_length = tf.shape(y_pred['duration_pred'])[1]
        
        # Dummy targets (in real implementation, these would be provided)
        duration_target = tf.ones((batch_size, seq_length, 1), dtype=tf.float32)
        pitch_target = tf.zeros((batch_size, seq_length, 1), dtype=tf.float32)
        energy_target = tf.zeros((batch_size, seq_length, 1), dtype=tf.float32)
        
        duration_loss = self.mae(duration_target, y_pred['duration_pred'])
        pitch_loss = self.mse(pitch_target, y_pred['pitch_pred'])
        energy_loss = self.mse(energy_target, y_pred['energy_pred'])
        
        total_loss = (self.mel_loss_weight * total_mel_loss +
                     self.duration_loss_weight * duration_loss +
                     self.pitch_loss_weight * pitch_loss +
                     self.energy_loss_weight * energy_loss)
        
        return total_loss


class BasicTTSLoss(tf.keras.losses.Loss):
    """Basic loss function for simple TTS models."""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.mse = tf.keras.losses.MeanSquaredError()
    
    def call(self, y_true, y_pred):
        """
        Compute basic MSE loss for TTS model.
        
        y_true: mel-spectrogram (batch_size, time_steps, n_mels)
        y_pred: predicted mel-spectrogram or dict containing model outputs
        """
        if isinstance(y_pred, dict):
            # If the model returns a dictionary, use the main output
            main_output_key = 'mel_output_refined' if 'mel_output_refined' in y_pred else 'mel_output'
            y_pred = y_pred.get(main_output_key, list(y_pred.values())[0])
        
        return self.mse(y_true, y_pred)

def load_ljspeech_dataset(
    split: str = "train", batch_size: int = 32, limit_samples: int = None
) -> tf.data.Dataset:
    """
    Load the LJSpeech dataset from TensorFlow Datasets.
    Optimized for efficient loading when limiting samples.
    """
    print(f"Loading LJSpeech dataset with split: {split}")
    
    try:
        import tensorflow_datasets as tfds
    except ImportError as e:
        print(f"Error importing tensorflow_datasets: {e}")
        print("Please install tensorflow_datasets: pip install tensorflow-datasets")
        raise

    # Optimize split for limited samples to avoid processing entire dataset
    if limit_samples:
        print(f"ğŸš€ åŠ¹ç‡çš„ãƒ¢ãƒ¼ãƒ‰: æœ€åˆã®{limit_samples}ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ãƒ­ãƒ¼ãƒ‰")
        optimized_split = f"{split}[:{limit_samples}]"
        print(f"ğŸ“Š ä½¿ç”¨split: {optimized_split}")
    else:
        optimized_split = split

    dataset, info = tfds.load(
        "ljspeech",
        split=optimized_split,
        with_info=True,
        as_supervised=True,
        data_dir="./datasets",
    )

    print(f"Dataset info: {info}")
    if limit_samples:
        print(f"âœ… åŠ¹ç‡çš„ãƒ­ãƒ¼ãƒ‰å®Œäº†: {limit_samples}ã‚µãƒ³ãƒ—ãƒ«")
    else:
        print(f"Number of examples: {info.splits[split].num_examples}")

    return dataset.batch(batch_size)


def preprocess_audio(audio: tf.Tensor) -> tf.Tensor:
    """
    Preprocess audio data for machine learning.
    """
    audio = tf.cast(audio, tf.float32) / 32768.0
    if len(audio.shape) > 1 and audio.shape[-1] > 1:
        audio = tf.reduce_mean(audio, axis=-1)
    return audio


def extract_mel_spectrogram(
    audio: tf.Tensor,
    sample_rate: int = 22050,
    n_mels: int = 80,
    n_fft: int = 1024,
    hop_length: int = 256,
) -> tf.Tensor:
    """
    Extract mel-spectrogram features from audio.
    """
    audio_np = audio.numpy()
    mel_spec = librosa.feature.melspectrogram(
        y=audio_np, sr=sample_rate, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length
    )
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_spec_db = mel_spec_db.T  # Transpose to [time, n_mels]
    return tf.convert_to_tensor(mel_spec_db, dtype=tf.float32)


def create_text_encoder(vocab_size: int = 10000) -> tf.keras.layers.TextVectorization:
    """
    Create a text encoder for processing transcriptions.
    """
    return tf.keras.layers.TextVectorization(
        max_tokens=vocab_size,
        output_sequence_length=MAX_FRAMES,  # Use MAX_FRAMES for output sequence length
        standardize="lower_and_strip_punctuation",
    )


class TTSModelTrainer(tf.keras.Model):
    """Universal wrapper class for TTS model training with custom train_step."""
    
    def __init__(self, tts_model, model_type: TTSModel = TTSModel.FASTSPEECH2, **kwargs):
        super().__init__(**kwargs)
        self.tts_model = tts_model
        self.model_type = model_type
        
        # Select appropriate loss function based on model type
        if model_type == TTSModel.FASTSPEECH2:
            self.loss_fn = FastSpeech2Loss()
        elif model_type == TTSModel.TRANSFORMER_TTS:
            from simple_transformer_tts import SimpleTransformerTTSLoss
            self.loss_fn = SimpleTransformerTTSLoss()
        elif model_type == TTSModel.VITS:
            from simple_vits import SimpleVITSLoss
            self.loss_fn = SimpleVITSLoss()
        else:
            self.loss_fn = BasicTTSLoss()
    
    def call(self, inputs, training=None):
        return self.tts_model(inputs, training=training)
    
    def train_step(self, data):
        x, y = data
        
        with tf.GradientTape() as tape:
            # Forward pass
            model_outputs = self.tts_model(x, training=True)
            
            # Compute loss
            loss = self.loss_fn(y, model_outputs)
        
        # Compute gradients and update weights
        gradients = tape.gradient(loss, self.tts_model.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.tts_model.trainable_variables))
        
        # For transformer_tts, skip metrics update due to shape mismatch
        # The loss function handles all necessary computations
        if self.model_type == TTSModel.TRANSFORMER_TTS:
            return {"loss": loss}
        
        # Get main output for metrics (for other models)
        if isinstance(model_outputs, dict):
            main_output = model_outputs.get('mel_output_refined', 
                                          model_outputs.get('mel_output', 
                                                          list(model_outputs.values())[0]))
        else:
            main_output = model_outputs
            
        # Update metrics (skip for transformer_tts to avoid shape issues)
        try:
            self.compiled_metrics.update_state(y, main_output)
            # Safely get metrics results
            metric_results = {}
            for m in self.metrics:
                try:
                    metric_results[m.name] = m.result()
                except:
                    # Skip metrics that can't be computed
                    pass
            return {"loss": loss, **metric_results}
        except Exception:
            # If metrics update fails, just return loss
            return {"loss": loss}


# Legacy alias for backward compatibility
FastSpeech2Trainer = TTSModelTrainer


def build_fastspeech2_model(
    vocab_size: int, mel_bins: int = 80, max_sequence_length: int = MAX_FRAMES
) -> tf.keras.Model:
    """
    Build FastSpeech 2 model for speech synthesis.
    """
    # Import FastSpeech 2 model
    from fastspeech2_model import FastSpeech2, create_fastspeech2_config
    
    # Create configuration for FastSpeech 2
    config = create_fastspeech2_config()
    config['vocab_size'] = vocab_size
    config['num_mels'] = mel_bins
    
    # Create FastSpeech 2 model
    fastspeech2_model = FastSpeech2(config)
    
    # Build the model by calling it with dummy input
    dummy_input = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.int32)
    _ = fastspeech2_model(dummy_input)
    
    # Wrap with trainer class
    model = TTSModelTrainer(fastspeech2_model, TTSModel.FASTSPEECH2)
    
    # Compile the model
    model.compile(
        optimizer='adam',
        metrics=['mae']
    )
    
    return model


def build_simple_transformer_model(
    vocab_size: int, mel_bins: int = 80, max_sequence_length: int = MAX_FRAMES
) -> tf.keras.Model:
    """
    Build a simple Transformer-based TTS model for comparison.
    This is a basic implementation for demonstration purposes.
    """
    # Input layer
    text_input = tf.keras.layers.Input(shape=(max_sequence_length,), name='text_input')
    
    # Embedding layer
    embedding = tf.keras.layers.Embedding(vocab_size, 256)(text_input)
    
    # Positional encoding (simplified)
    pos_encoding = tf.keras.layers.Dense(256, activation='linear')(embedding)
    x = embedding + pos_encoding
    
    # Transformer blocks (simplified)
    for _ in range(4):
        # Multi-head attention
        attention = tf.keras.layers.MultiHeadAttention(
            num_heads=8, key_dim=32, dropout=0.1
        )(x, x)
        x = tf.keras.layers.LayerNormalization()(x + attention)
        
        # Feed forward
        ffn = tf.keras.layers.Dense(1024, activation='relu')(x)
        ffn = tf.keras.layers.Dense(256)(ffn)
        x = tf.keras.layers.LayerNormalization()(x + ffn)
    
    # Output projection to mel-spectrogram
    mel_output = tf.keras.layers.Dense(mel_bins, activation='linear', name='mel_output')(x)
    
    # Create model
    simple_model = tf.keras.Model(inputs=text_input, outputs=mel_output)
    
    # Wrap with trainer class
    model = TTSModelTrainer(simple_model, TTSModel.TRANSFORMER_TTS)
    
    # Compile the model
    model.compile(
        optimizer='adam',
        metrics=['mae']
    )
    
    return model


def build_text_to_spectrogram_model(
    vocab_size: int, 
    mel_bins: int = 80, 
    max_sequence_length: int = MAX_FRAMES,
    model_type: TTSModel = TTSModel.FASTSPEECH2
) -> tf.keras.Model:
    """
    Build TTS model based on the specified model type.
    
    Args:
        vocab_size: Size of the vocabulary
        mel_bins: Number of mel-spectrogram bins
        max_sequence_length: Maximum sequence length
        model_type: Type of TTS model to build
        
    Returns:
        Compiled TTS model ready for training
    """
    print(f"ğŸ—ï¸  æ§‹ç¯‰ä¸­ã®ãƒ¢ãƒ‡ãƒ«: {model_type.value}")
    
    if model_type == TTSModel.FASTSPEECH2:
        return build_fastspeech2_model(vocab_size, mel_bins, max_sequence_length)
    elif model_type == TTSModel.TRANSFORMER_TTS:
        # Import the simple transformer TTS model
        from simple_transformer_tts import build_simple_transformer_tts_model, create_simple_transformer_config
        config = create_simple_transformer_config()
        config['vocab_size'] = vocab_size
        config['n_mels'] = mel_bins
        return build_simple_transformer_tts_model(vocab_size, mel_bins, config)
    elif model_type == TTSModel.VITS:
        # Import the Simple VITS model
        from simple_vits import build_simple_vits_model, create_simple_vits_config
        config = create_simple_vits_config()
        config['vocab_size'] = vocab_size
        config['n_mels'] = mel_bins
        return build_simple_vits_model(vocab_size, mel_bins, config)
    elif model_type == TTSModel.TACOTRON2:
        # Placeholder for future Tacotron 2 implementation
        raise NotImplementedError("Tacotron 2 ãƒ¢ãƒ‡ãƒ«ã¯å°†æ¥ã®å®Ÿè£…äºˆå®šã§ã™")
    else:
        raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")


def get_model_config(model_type: TTSModel) -> Dict[str, Any]:
    """Get model-specific configuration."""
    if model_type == TTSModel.FASTSPEECH2:
        from fastspeech2_model import create_fastspeech2_config
        return create_fastspeech2_config()
    elif model_type == TTSModel.TRANSFORMER_TTS:
        from simple_transformer_tts import create_simple_transformer_config
        return create_simple_transformer_config()
    elif model_type == TTSModel.VITS:
        from simple_vits import create_simple_vits_config
        return create_simple_vits_config()
    else:
        return {}


def visualize_audio_and_spectrogram(
    audio: tf.Tensor, text: str, sample_rate: int = 22050, save_path: str | None = None
):
    """
    Visualize audio waveform and mel-spectrogram.
    """
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    audio_np = audio.numpy()
    time = np.arange(len(audio_np)) / sample_rate
    ax1.plot(time, audio_np)
    ax1.set_title(f'Waveform - Text: "{text[:50]}..."')
    ax1.set_xlabel("Time (s)")
    ax1.set_ylabel("Amplitude")
    ax1.grid(True)

    mel_spec_db = extract_mel_spectrogram(audio)
    mel_spec_db_np = mel_spec_db.numpy()
    librosa.display.specshow(
        mel_spec_db_np.T,  # Transpose back for display
        sr=sample_rate,
        hop_length=256,
        x_axis="time",
        y_axis="mel",
        ax=ax2,
    )
    ax2.set_title("Mel-Spectrogram")
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"Plot saved to {save_path}")
    plt.close()  # ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤ºã›ãšã«é–‰ã˜ã‚‹


class TrainingPlotCallback(tf.keras.callbacks.Callback):
    """
    å­¦ç¿’éç¨‹ï¼ˆepochã€lossã€MAEï¼‰ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚°ãƒ©ãƒ•åŒ–ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """
    
    def __init__(self, model_output_dir=None, model_type=TTSModel.FASTSPEECH2):
        super().__init__()
        self.model_type = model_type
        
        # ã‚°ãƒ©ãƒ•ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        if model_output_dir:
            self.plot_dir = os.path.join(model_output_dir, "training_plots")
        else:
            self.plot_dir = os.path.join(BASE_OUTPUT_DIR, model_type.value, "training_plots")
        
        os.makedirs(self.plot_dir, exist_ok=True)
        
        # å­¦ç¿’å±¥æ­´ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        self.epochs = []
        self.train_losses = []
        self.train_maes = []
        self.val_losses = []
        self.val_maes = []
        
        # ã‚°ãƒ©ãƒ•ã®ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        plt.style.use('default')
        
        print(f"ğŸ“Š å­¦ç¿’éç¨‹ã‚°ãƒ©ãƒ•åŒ–æ©Ÿèƒ½ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        print(f"ğŸ“ ã‚°ãƒ©ãƒ•ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.plot_dir}")
    
    def on_epoch_end(self, epoch, logs=None):
        """ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã«å­¦ç¿’æ›²ç·šã‚’æ›´æ–°"""
        if logs is None:
            logs = {}
        
        # ã‚¨ãƒãƒƒã‚¯ç•ªå·ã‚’è¨˜éŒ²ï¼ˆ1ã‹ã‚‰å§‹ã¾ã‚‹ï¼‰
        current_epoch = epoch + 1
        self.epochs.append(current_epoch)
        
        # æå¤±ã¨MAEã‚’è¨˜éŒ²
        train_loss = logs.get('loss', 0)
        train_mae = logs.get('mae', logs.get('mean_absolute_error', 0))
        val_loss = logs.get('val_loss', None)
        val_mae = logs.get('val_mae', logs.get('val_mean_absolute_error', None))
        
        self.train_losses.append(train_loss)
        self.train_maes.append(train_mae)
        
        if val_loss is not None:
            self.val_losses.append(val_loss)
        if val_mae is not None:
            self.val_maes.append(val_mae)
        
        # ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆãƒ»ä¿å­˜
        self._create_training_plots(current_epoch)
        
        print(f"ğŸ“Š ã‚¨ãƒãƒƒã‚¯ {current_epoch}: Loss={train_loss:.4f}, MAE={train_mae:.4f}")
    
    def _create_training_plots(self, current_epoch):
        """å­¦ç¿’æ›²ç·šã‚°ãƒ©ãƒ•ã‚’ä½œæˆãƒ»ä¿å­˜"""
        try:
            # ãƒ•ã‚£ã‚®ãƒ¥ã‚¢ã‚µã‚¤ã‚ºã‚’è¨­å®š
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'{self.model_type.value.upper()} Training Progress - Epoch {current_epoch}', 
                        fontsize=16, fontweight='bold')
            
            # 1. Lossæ›²ç·š
            ax1.plot(self.epochs, self.train_losses, 'b-', label='Training Loss', linewidth=2)
            if self.val_losses:
                ax1.plot(self.epochs, self.val_losses, 'r-', label='Validation Loss', linewidth=2)
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Loss')
            ax1.set_title('Training and Validation Loss')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            if len(self.epochs) > 1:
                ax1.set_xlim(1, max(self.epochs))
            
            # 2. MAEæ›²ç·š
            ax2.plot(self.epochs, self.train_maes, 'g-', label='Training MAE', linewidth=2)
            if self.val_maes:
                ax2.plot(self.epochs, self.val_maes, 'orange', label='Validation MAE', linewidth=2)
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Mean Absolute Error')
            ax2.set_title('Training and Validation MAE')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            if len(self.epochs) > 1:
                ax2.set_xlim(1, max(self.epochs))
            
            # 3. Lossæœ€æ–°20ã‚¨ãƒãƒƒã‚¯ã®è©³ç´°
            recent_epochs = self.epochs[-20:] if len(self.epochs) > 20 else self.epochs
            recent_train_losses = self.train_losses[-20:] if len(self.train_losses) > 20 else self.train_losses
            recent_val_losses = self.val_losses[-20:] if len(self.val_losses) > 20 else self.val_losses
            
            ax3.plot(recent_epochs, recent_train_losses, 'b-', label='Training Loss', linewidth=2, marker='o')
            if recent_val_losses:
                ax3.plot(recent_epochs, recent_val_losses, 'r-', label='Validation Loss', linewidth=2, marker='s')
            ax3.set_xlabel('Epoch')
            ax3.set_ylabel('Loss')
            ax3.set_title('Recent Loss (Last 20 Epochs)')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            # 4. å­¦ç¿’çµ±è¨ˆæƒ…å ±
            ax4.axis('off')
            stats_text = f"""Training Statistics (Epoch {current_epoch})
             
Current Metrics:
â€¢ Training Loss: {self.train_losses[-1]:.6f}
â€¢ Training MAE: {self.train_maes[-1]:.6f}"""
            
            if self.val_losses:
                stats_text += f"\nâ€¢ Validation Loss: {self.val_losses[-1]:.6f}"
            if self.val_maes:
                stats_text += f"\nâ€¢ Validation MAE: {self.val_maes[-1]:.6f}"
            
            if len(self.train_losses) > 1:
                best_train_loss = min(self.train_losses)
                best_train_loss_epoch = self.epochs[self.train_losses.index(best_train_loss)]
                stats_text += f"""

Best Performance:
â€¢ Best Training Loss: {best_train_loss:.6f} (Epoch {best_train_loss_epoch})"""
                
                if len(self.train_maes) > 1:
                    best_train_mae = min(self.train_maes)
                    best_train_mae_epoch = self.epochs[self.train_maes.index(best_train_mae)]
                    stats_text += f"\nâ€¢ Best Training MAE: {best_train_mae:.6f} (Epoch {best_train_mae_epoch})"
            
            ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=11,
                    verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
            
            # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
            plt.tight_layout()
            
            # ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_filename = f"training_progress_epoch_{current_epoch:04d}_{timestamp}.png"
            plot_path = os.path.join(self.plot_dir, plot_filename)
            
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()  # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’é˜²ããŸã‚
            
            # æœ€æ–°ã®ã‚°ãƒ©ãƒ•ã‚’å›ºå®šåã§ã‚‚ä¿å­˜ï¼ˆå¸¸ã«æœ€æ–°çŠ¶æ…‹ã‚’ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ï¼‰
            latest_plot_path = os.path.join(self.plot_dir, "latest_training_progress.png")
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'{self.model_type.value.upper()} Training Progress - Epoch {current_epoch}', 
                        fontsize=16, fontweight='bold')
            
            # åŒã˜ã‚°ãƒ©ãƒ•ã‚’å†ä½œæˆ
            ax1.plot(self.epochs, self.train_losses, 'b-', label='Training Loss', linewidth=2)
            if self.val_losses:
                ax1.plot(self.epochs, self.val_losses, 'r-', label='Validation Loss', linewidth=2)
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Loss')
            ax1.set_title('Training and Validation Loss')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            if len(self.epochs) > 1:
                ax1.set_xlim(1, max(self.epochs))
            
            ax2.plot(self.epochs, self.train_maes, 'g-', label='Training MAE', linewidth=2)
            if self.val_maes:
                ax2.plot(self.epochs, self.val_maes, 'orange', label='Validation MAE', linewidth=2)
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Mean Absolute Error')
            ax2.set_title('Training and Validation MAE')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            if len(self.epochs) > 1:
                ax2.set_xlim(1, max(self.epochs))
            
            ax3.plot(recent_epochs, recent_train_losses, 'b-', label='Training Loss', linewidth=2, marker='o')
            if recent_val_losses:
                ax3.plot(recent_epochs, recent_val_losses, 'r-', label='Validation Loss', linewidth=2, marker='s')
            ax3.set_xlabel('Epoch')
            ax3.set_ylabel('Loss')
            ax3.set_title('Recent Loss (Last 20 Epochs)')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            ax4.axis('off')
            ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=11,
                    verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
            
            plt.tight_layout()
            plt.savefig(latest_plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"  ğŸ“Š å­¦ç¿’æ›²ç·šã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {plot_filename}")
            
        except Exception as e:
            print(f"âŒ ã‚°ãƒ©ãƒ•ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
    
    def save_training_history(self):
        """å­¦ç¿’å±¥æ­´ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
        try:
            history_data = {
                'epochs': self.epochs,
                'train_losses': self.train_losses,
                'train_maes': self.train_maes,
                'val_losses': self.val_losses,
                'val_maes': self.val_maes,
                'model_type': self.model_type.value,
                'saved_at': datetime.now().isoformat()
            }
            
            history_path = os.path.join(self.plot_dir, "training_history.json")
            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ å­¦ç¿’å±¥æ­´ã‚’ä¿å­˜: {history_path}")
            
        except Exception as e:
            print(f"âŒ å­¦ç¿’å±¥æ­´ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

class SynthesisCallback(tf.keras.callbacks.Callback):
    def __init__(self, text_encoder, n_fft, hop_length, sample_rate=22050, inference_text=None, model_type=TTSModel.FASTSPEECH2, model_output_dir=None):
        super().__init__()
        self.text_encoder = text_encoder
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.sample_rate = sample_rate
        self.model_type = model_type
        # éŸ³å£°åˆæˆã«ä½¿ç”¨ã™ã‚‹ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ï¼‰
        self.inference_text = inference_text if inference_text else "This is a test of the model at the end of each epoch."
        
        # ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®šï¼ˆmodel_output_dirãŒæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ï¼‰
        if model_output_dir:
            self.epoch_samples_dir = os.path.join(model_output_dir, "epoch_samples")
        else:
            # æ—§æ–¹å¼ã®äº’æ›æ€§ç¶­æŒ
            self.epoch_samples_dir = os.path.join(BASE_OUTPUT_DIR, model_type.value, "epoch_samples")
        
        os.makedirs(self.epoch_samples_dir, exist_ok=True)
        
        # æ™‚é–“äºˆæ¸¬ç”¨ã®å¤‰æ•°
        self.training_start_time = None
        self.epoch_start_time = None
        self.epoch_times = []  # å„ã‚¨ãƒãƒƒã‚¯ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨˜éŒ²
        self.total_epochs = None

    def on_train_begin(self, logs=None):
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹æ™‚ã«é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²ã—ã€ç·ã‚¨ãƒãƒƒã‚¯æ•°ã‚’è¨­å®š"""
        self.training_start_time = time.time()
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ç·ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å–å¾—
        self.total_epochs = self.params.get('epochs', 500)
        start_time_str = datetime.fromtimestamp(self.training_start_time).strftime('%Y-%m-%d %H:%M:%S')
        print(f"\nğŸš€ å­¦ç¿’é–‹å§‹æ™‚åˆ»: {start_time_str}")
        print(f"ğŸ“Š ç·ã‚¨ãƒãƒƒã‚¯æ•°: {self.total_epochs}")

    def on_epoch_begin(self, epoch, logs=None):
        """Update global variables at the start of each epoch."""
        global current_epoch, current_model, current_text_encoder
        current_epoch = epoch
        current_model = self.model
        current_text_encoder = self.text_encoder
        
        # ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
        self.epoch_start_time = time.time()
        epoch_start_str = datetime.fromtimestamp(self.epoch_start_time).strftime('%H:%M:%S')
        print(f"\nğŸš€ ã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{self.total_epochs} ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™... (é–‹å§‹æ™‚åˆ»: {epoch_start_str})")

    def on_epoch_end(self, epoch, logs=None):
        """Generate audio sample, save state, and predict completion time at the end of each epoch."""
        global training_interrupted
        
        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²
        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - self.epoch_start_time
        self.epoch_times.append(epoch_duration)
        
        # æ™‚é–“äºˆæ¸¬ã®è¨ˆç®—
        completed_epochs = epoch + 1
        remaining_epochs = self.total_epochs - completed_epochs
        
        # å¹³å‡ã‚¨ãƒãƒƒã‚¯æ™‚é–“ã‚’è¨ˆç®—
        avg_epoch_time = sum(self.epoch_times) / len(self.epoch_times)
        
        # æ®‹ã‚Šæ™‚é–“ã‚’è¨ˆç®—
        estimated_remaining_time = remaining_epochs * avg_epoch_time
        estimated_completion_time = epoch_end_time + estimated_remaining_time
        
        # æ™‚é–“æƒ…å ±ã‚’è¡¨ç¤º
        epoch_duration_min = epoch_duration / 60
        avg_epoch_time_min = avg_epoch_time / 60
        remaining_time_min = estimated_remaining_time / 60
        
        completion_time_str = datetime.fromtimestamp(estimated_completion_time).strftime('%Y-%m-%d %H:%M:%S')
        
        print(f"\nâ±ï¸  === ã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{self.total_epochs} æ™‚é–“æƒ…å ± ===")
        print(f"ğŸ“ˆ ä»Šå›ã®ã‚¨ãƒãƒƒã‚¯å®Ÿè¡Œæ™‚é–“: {epoch_duration_min:.1f}åˆ†")
        print(f"ğŸ“Š å¹³å‡ã‚¨ãƒãƒƒã‚¯å®Ÿè¡Œæ™‚é–“: {avg_epoch_time_min:.1f}åˆ†")
        
        if remaining_epochs > 0:
            print(f"â³ æ®‹ã‚Šã‚¨ãƒãƒƒã‚¯æ•°: {remaining_epochs}")
            print(f"ğŸ• æ¨å®šæ®‹ã‚Šæ™‚é–“: {remaining_time_min:.1f}åˆ†")
            print(f"ğŸ æ¨å®šå®Œäº†æ™‚åˆ»: {completion_time_str}")
        else:
            total_training_time = (epoch_end_time - self.training_start_time) / 60
            print(f"ğŸ‰ å…¨å­¦ç¿’å®Œäº†ï¼ç·å­¦ç¿’æ™‚é–“: {total_training_time:.1f}åˆ†")
        
        print("=" * 50)
        
        if training_interrupted:
            print("\nâš ï¸  ä¸­æ–­ãŒè¦æ±‚ã•ã‚Œã¾ã—ãŸã€‚éŸ³å£°ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return
            
        print(f"\n\n--- ã‚¨ãƒãƒƒã‚¯ {epoch + 1} çµ‚äº†æ™‚ã®éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ ---")

        try:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            text_vec = self.text_encoder([self.inference_text])

            # ãƒ¢ãƒ‡ãƒ«ã§ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’äºˆæ¸¬
            model_output = self.model.predict(text_vec)
            
            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦é©åˆ‡ãªå‡ºåŠ›ã‚’å–å¾—
            if isinstance(model_output, dict):
                if self.model_type in [TTSModel.FASTSPEECH2, TTSModel.TRANSFORMER_TTS, TTSModel.VITS]:
                    predicted_mel_spec = model_output['mel_output_refined']
                else:
                    # ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®å ´åˆã€åˆ©ç”¨å¯èƒ½ãªå‡ºåŠ›ã‚­ãƒ¼ã‹ã‚‰é¸æŠ
                    predicted_mel_spec = model_output.get('mel_output_refined', 
                                                        model_output.get('mel_output',
                                                                        list(model_output.values())[0]))
            else:
                predicted_mel_spec = model_output

            # ãƒãƒƒãƒæ¬¡å…ƒã‚’å‰Šé™¤ã—ã€numpyé…åˆ—ã«å¤‰æ›
            predicted_mel_spec_np = predicted_mel_spec[0]

            # ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’ãƒ‡ã‚·ãƒ™ãƒ«ã‹ã‚‰ãƒ‘ãƒ¯ãƒ¼ã«å¤‰æ›
            predicted_mel_spec_db_t = predicted_mel_spec_np.T
            power_spec = librosa.db_to_power(predicted_mel_spec_db_t)

            # Griffin-Limã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§éŸ³å£°ã‚’å¾©å…ƒ
            generated_audio = librosa.feature.inverse.mel_to_audio(
                power_spec,
                sr=self.sample_rate,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
            )

            # ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ã‚’ä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_audio_path = f"{self.epoch_samples_dir}/epoch_{epoch + 1}_{timestamp}.wav"
            sf.write(output_audio_path, generated_audio, self.sample_rate)
            print(f"ğŸµ ã‚¨ãƒãƒƒã‚¯ {epoch + 1} ã®éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜: {output_audio_path}")

        except Exception as e:
            print(f"âŒ ã‚¨ãƒãƒƒã‚¯ {epoch + 1} ã®éŸ³å£°ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()

    def on_train_end(self, logs=None):
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµ‚äº†æ™‚ã®æœ€çµ‚æƒ…å ±ã‚’è¡¨ç¤º"""
        if self.training_start_time is not None:
            total_training_time = (time.time() - self.training_start_time) / 60
            end_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            print(f"\nğŸ‰ === å­¦ç¿’å®Œäº†æƒ…å ± ===")
            print(f"ğŸ å­¦ç¿’çµ‚äº†æ™‚åˆ»: {end_time_str}")
            print(f"â±ï¸  ç·å­¦ç¿’æ™‚é–“: {total_training_time:.1f}åˆ†")
            print(f"ğŸ“Š å®Ÿè¡Œã•ã‚ŒãŸã‚¨ãƒãƒƒã‚¯æ•°: {len(self.epoch_times)}")
            if self.epoch_times:
                avg_time = sum(self.epoch_times) / len(self.epoch_times) / 60
                print(f"ğŸ“ˆ å¹³å‡ã‚¨ãƒãƒƒã‚¯æ™‚é–“: {avg_time:.1f}åˆ†")
            print("=" * 50)


def save_training_state(epoch, text_encoder, model, model_type=TTSModel.FASTSPEECH2, limit_samples=None, mode="mini"):
    """Save training state including epoch number, text encoder, model type, and training configuration."""
    try:
        os.makedirs(CHECKPOINT_DIR, exist_ok=True)

        print(f"ğŸ’¾ ã‚¨ãƒãƒƒã‚¯ {epoch} ã®çŠ¶æ…‹ã‚’ä¿å­˜ä¸­...")
        
        # Save model weights instead of full model to avoid serialization issues
        model_weights_path = os.path.join(CHECKPOINT_DIR, "model.weights.h5")
        model.save_weights(model_weights_path)
        print(f"  âœ… ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’ä¿å­˜: {model_weights_path}")

        # Save text encoder vocabulary
        vocab = text_encoder.get_vocabulary()
        vocab_path = os.path.join(CHECKPOINT_DIR, "vocabulary.json")
        with open(vocab_path, "w", encoding='utf-8') as f:
            json.dump(vocab, f, ensure_ascii=False, indent=2)
        print(f"  âœ… èªå½™ã‚’ä¿å­˜: {vocab_path}")

        # Determine sample directory name
        if limit_samples is not None:
            sample_dir = f"samples_{limit_samples}"
        elif mode == 'full':
            sample_dir = "full_dataset"
        else:  # mini mode
            sample_dir = "samples_10"

        # Save training state with timestamp, model type, and training configuration
        training_state = {
            "epoch": epoch,
            "model_type": model_type.value,
            "vocab_size": text_encoder.vocabulary_size(),
            "max_tokens": text_encoder._max_tokens,
            "output_sequence_length": text_encoder._output_sequence_length,
            "standardize": text_encoder._standardize,
            "limit_samples": limit_samples,
            "mode": mode,
            "sample_dir": sample_dir,
            "saved_at": datetime.now().isoformat(),
            "tensorflow_version": tf.__version__,
            "checkpoint_version": "3.2"
        }
        
        # Create backup of previous state
        if os.path.exists(TRAINING_STATE_PATH):
            backup_path = TRAINING_STATE_PATH + ".backup"
            os.rename(TRAINING_STATE_PATH, backup_path)
            print(f"  ğŸ“‹ å‰å›ã®çŠ¶æ…‹ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path}")
        
        with open(TRAINING_STATE_PATH, "w", encoding='utf-8') as f:
            json.dump(training_state, f, ensure_ascii=False, indent=2)
        print(f"  âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’ä¿å­˜: {TRAINING_STATE_PATH}")

        print(f"ğŸ’¾ ã‚¨ãƒãƒƒã‚¯ {epoch} ã®ä¿å­˜å®Œäº†")
        
    except Exception as e:
        print(f"âŒ çŠ¶æ…‹ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        raise


def load_training_state():
    """Load training state and return epoch number, text encoder, model, model type, and training config if available."""
    print("ğŸ” ä¿å­˜ã•ã‚ŒãŸçŠ¶æ…‹ã‚’ç¢ºèªä¸­...")
    
    if not os.path.exists(TRAINING_STATE_PATH):
        print("â„¹ï¸  ä¿å­˜ã•ã‚ŒãŸçŠ¶æ…‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æœ€åˆã‹ã‚‰é–‹å§‹ã—ã¾ã™ã€‚")
        return 0, None, None, TTSModel.FASTSPEECH2, None

    try:
        # Load training state
        print(f"ğŸ“‚ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿ä¸­: {TRAINING_STATE_PATH}")
        with open(TRAINING_STATE_PATH, "r", encoding='utf-8') as f:
            training_state = json.load(f)
        
        # Validate checkpoint version compatibility
        checkpoint_version = training_state.get("checkpoint_version", "1.0")
        print(f"  ğŸ“‹ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³: {checkpoint_version}")
        
        # Get model type from saved state (default to FastSpeech2 for older checkpoints)
        model_type_str = training_state.get("model_type", "fastspeech2")
        try:
            model_type = TTSModel(model_type_str)
            print(f"  ğŸ—ï¸  ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type.value}")
        except ValueError:
            print(f"  âš ï¸  ä¸æ˜ãªãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— '{model_type_str}'ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®FastSpeech2ã‚’ä½¿ç”¨")
            model_type = TTSModel.FASTSPEECH2
        
        # Extract training configuration
        training_config = {
            'limit_samples': training_state.get('limit_samples'),
            'mode': training_state.get('mode', 'mini'),
            'sample_dir': training_state.get('sample_dir', 'samples_10')
        }
        
        if training_state.get("tensorflow_version"):
            print(f"  ğŸ”§ ä¿å­˜æ™‚ã®TensorFlowãƒãƒ¼ã‚¸ãƒ§ãƒ³: {training_state['tensorflow_version']}")
            print(f"  ğŸ”§ ç¾åœ¨ã®TensorFlowãƒãƒ¼ã‚¸ãƒ§ãƒ³: {tf.__version__}")
        
        saved_at = training_state.get("saved_at", "ä¸æ˜")
        print(f"  â° ä¿å­˜æ™‚åˆ»: {saved_at}")

        # Check if model weights file exists
        model_weights_path = os.path.join(CHECKPOINT_DIR, "model.weights.h5")
        if not os.path.exists(model_weights_path):
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_weights_path}")
            return 0, None, None, model_type, training_config

        # Load vocabulary
        vocab_path = os.path.join(CHECKPOINT_DIR, "vocabulary.json")
        if not os.path.exists(vocab_path):
            print(f"âŒ èªå½™ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {vocab_path}")
            return 0, None, None, model_type, training_config
            
        print(f"ğŸ“– èªå½™ã‚’èª­ã¿è¾¼ã¿ä¸­: {vocab_path}")
        with open(vocab_path, "r", encoding='utf-8') as f:
            vocab = json.load(f)

        # Recreate text encoder
        print("ğŸ”¤ ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’å†æ§‹ç¯‰ä¸­...")
        text_encoder = tf.keras.layers.TextVectorization(
            max_tokens=training_state["max_tokens"],
            output_sequence_length=training_state["output_sequence_length"],
            standardize=training_state["standardize"],
        )
        text_encoder.set_vocabulary(vocab)
        print(f"  âœ… èªå½™ã‚µã‚¤ã‚º: {len(vocab)}")

        # Create new model with same architecture
        print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’å†æ§‹ç¯‰ä¸­...")
        model = build_text_to_spectrogram_model(
            vocab_size=training_state["vocab_size"],
            mel_bins=80,
            max_sequence_length=training_state["output_sequence_length"],
            model_type=model_type
        )
        
        # Load weights
        print(f"âš–ï¸  ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_weights_path}")
        model.load_weights(model_weights_path)
        print(f"  âœ… ãƒ¢ãƒ‡ãƒ«é‡ã¿èª­ã¿è¾¼ã¿å®Œäº†")

        epoch = training_state["epoch"]
        print(f"ğŸ¯ ã‚¨ãƒãƒƒã‚¯ {epoch} ã‹ã‚‰å†é–‹ã—ã¾ã™")
        print("=" * 50)
        
        return epoch, text_encoder, model, model_type, training_config

    except Exception as e:
        print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("âš ï¸  æœ€åˆã‹ã‚‰é–‹å§‹ã—ã¾ã™...")
        import traceback
        traceback.print_exc()
        
        # Try to use backup if available
        backup_path = TRAINING_STATE_PATH + ".backup"
        if os.path.exists(backup_path):
            print(f"ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒã‚’è©¦è¡Œä¸­: {backup_path}")
            try:
                os.rename(backup_path, TRAINING_STATE_PATH)
                return load_training_state()  # Recursive call with backup
            except Exception as backup_error:
                print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©å…ƒã‚‚å¤±æ•—ã—ã¾ã—ãŸ: {backup_error}")
        
        return 0, None, None, TTSModel.FASTSPEECH2, None


def main():
    """Main function to run the LJSpeech learning script."""
    import argparse
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='LJSpeech TTS Training Script with Model Selection')
    parser.add_argument('--mode', choices=['full', 'mini'], default='mini',
                       help='Training mode: full (entire dataset) or mini (first 10 samples)')
    parser.add_argument('--limit-samples', type=int, default=None,
                       help='Limit number of samples for training (overrides mode)')
    parser.add_argument('--epochs', type=int, default=2000,
                       help='Number of epochs to train (default: 2000)')
    parser.add_argument('--model', choices=['fastspeech2', 'transformer_tts'], default='fastspeech2',
                       help='TTS model type to use (default: fastspeech2)')
    args = parser.parse_args()
    
    # Convert model argument to enum
    model_type = TTSModel(args.model)
    
    # Set training parameters based on mode
    if args.limit_samples is not None:
        limit_samples = args.limit_samples
        mode_desc = f"ã‚«ã‚¹ã‚¿ãƒ ({limit_samples}ã‚µãƒ³ãƒ—ãƒ«)"
    elif args.mode == 'full':
        limit_samples = None
        mode_desc = "ãƒ•ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
    else:  # mini mode
        limit_samples = 10
        mode_desc = "ãƒŸãƒ‹ãƒ¢ãƒ¼ãƒ‰(10ã‚µãƒ³ãƒ—ãƒ«)"
    
    print("=== LJSpeech TTS éŸ³å£°åˆæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ ===")
    print(f"ğŸ—ï¸  ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_type.value}")
    print(f"ğŸ“Š å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰: {mode_desc}")
    print(f"ğŸ”„ ã‚¨ãƒãƒƒã‚¯æ•°: {args.epochs}")
    print(f"ğŸ• é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)
    
    try:
        # Setup model-specific paths
        model_output_dir = setup_model_paths(model_type, limit_samples, args.mode)
        
        # Check for existing checkpoint
        start_epoch, text_encoder, model, saved_model_type, training_config = load_training_state()

        # Check if the saved model type matches the requested model type and training configuration
        if start_epoch > 0 and training_config:
            config_matches = True
            if saved_model_type != model_type:
                config_matches = False
                print(f"âš ï¸  ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— ({saved_model_type.value}) ã¨è¦æ±‚ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— ({model_type.value}) ãŒç•°ãªã‚Šã¾ã™")
            
            # Check if training configuration matches
            saved_limit_samples = training_config.get('limit_samples')
            saved_mode = training_config.get('mode', 'mini')
            
            if saved_limit_samples != limit_samples or saved_mode != args.mode:
                config_matches = False
                print(f"âš ï¸  ä¿å­˜ã•ã‚ŒãŸè¨­å®š (samples: {saved_limit_samples}, mode: {saved_mode}) ã¨è¦æ±‚ã•ã‚ŒãŸè¨­å®š (samples: {limit_samples}, mode: {args.mode}) ãŒç•°ãªã‚Šã¾ã™")
            
            if config_matches:
                print(f"ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ã‚¨ãƒãƒƒã‚¯ {start_epoch + 1} ã‹ã‚‰å†é–‹ã—ã¾ã™")
                print(f"ğŸ“‹ ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {saved_model_type.value}")
                print(f"ğŸ“‹ ä¿å­˜ã•ã‚ŒãŸè¨­å®š: samples={saved_limit_samples}, mode={saved_mode}")
            else:
                print("ğŸ†• è¨­å®šãŒç•°ãªã‚‹ãŸã‚ã€æ–°è¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆæ—¢å­˜ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ç„¡è¦–ã•ã‚Œã¾ã™ï¼‰")
                start_epoch, text_encoder, model = 0, None, None
        else:
            print("ğŸ†• æ–°è¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™")

        # Load dataset
        print("\n=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ ===")
        if limit_samples:
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¶é™: æœ€åˆã®{limit_samples}ã‚µãƒ³ãƒ—ãƒ«")
        else:
            print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: ãƒ•ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½¿ç”¨")
        dataset = load_ljspeech_dataset(split="train", batch_size=1, limit_samples=limit_samples)
        
        # æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        print("ğŸ“ æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ä¸­...")
        first_text = None
        for text, audio in dataset.take(1):
            first_text = text[0].numpy().decode('utf-8')
            print(f"ğŸ¯ æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ: '{first_text}'")
            break
        
        # æ¨è«–ç”¨ãƒ†ã‚­ã‚¹ãƒˆã®è¨­å®š
        if args.mode == 'mini':
            inference_text = first_text
            print(f"ğŸ¤ ãƒŸãƒ‹ãƒ¢ãƒ¼ãƒ‰: æ¨è«–ãƒ†ã‚­ã‚¹ãƒˆã¯æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã¨åŒã˜")
        else:
            inference_text = first_text if first_text else f"This is a test of the {model_type.value} model."
            print(f"ğŸ¤ æ¨è«–ç”¨ãƒ†ã‚­ã‚¹ãƒˆ: '{inference_text}'")

        # Print dataset size before preparation
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’ç¢ºèªä¸­...")
        num_before = (
            dataset.unbatch()
            .reduce(tf.constant(0, dtype=tf.int64), lambda x, _: x + 1)
            .numpy()
        )
        print(f"ğŸ“ˆ å‰å‡¦ç†å‰ã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {num_before:,}")

        # Text processing setup
        print("\n=== ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç† ===")
        if text_encoder is None:
            print("ğŸ”¤ æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆä¸­...")
            text_encoder = create_text_encoder(vocab_size=10000)
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§èªå½™ã‚’æ§‹ç¯‰
            print("ğŸ“š èªå½™ã‚’æ§‹ç¯‰ä¸­...")
            # 10ã‚µãƒ³ãƒ—ãƒ«ã—ã‹ãªã„ãŸã‚ã€ã™ã¹ã¦ã‚’ä½¿ç”¨
            example_texts = dataset.unbatch().map(lambda text, audio: text)
            text_encoder.adapt(example_texts)
        else:
            print("â™»ï¸  ä¿å­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨")
            
        print(f"ğŸ“– èªå½™ã‚µã‚¤ã‚º: {text_encoder.vocabulary_size():,}")

        # Build model
        print("\n=== ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ===")
        if model is None:
            print("ğŸ¤– æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ä¸­...")
            model = build_text_to_spectrogram_model(
                vocab_size=text_encoder.vocabulary_size(),
                mel_bins=80,
                max_sequence_length=MAX_FRAMES,
                model_type=model_type
            )
        else:
            print("â™»ï¸  ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
            
        print("ğŸ“‹ ãƒ¢ãƒ‡ãƒ«æ§‹é€ :")
        model.summary()

        # --- Training Part ---
        print("\n=== ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ ===")

        # ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ è¨ˆç®—ã«å¿…è¦ãªæœ€å°ã®éŸ³å£°é•·ã‚’å®šç¾©
        N_FFT = 1024
        HOP_LENGTH = 256

        def filter_short_audio(text, audio):
            """
            éŸ³å£°ã®é•·ã•ãŒn_fftã‚ˆã‚ŠçŸ­ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é™¤å¤–ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°ã€‚
            """
            return tf.shape(audio)[0] > N_FFT

        def py_extract_mel_spectrogram_wrapper(audio):
            # ã“ã®é–¢æ•°ã¯tf.py_functionå†…ã§å‘¼ã°ã‚Œã‚‹
            return extract_mel_spectrogram(audio, n_fft=N_FFT, hop_length=HOP_LENGTH)

        def prepare_for_training(text, audio):
            """
            Prepare data for text-to-spectrogram training.
            Input: text vector
            Output: mel-spectrogram
            """
            audio_processed = preprocess_audio(audio)
            mel_spec = tf.py_function(
                func=py_extract_mel_spectrogram_wrapper,
                inp=[audio_processed],
                Tout=tf.float32,
            )
            mel_spec.set_shape((None, 80))  # Set shape for Keras
            text_vec = text_encoder(text)
            # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’MAX_FRAMESãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/åˆ‡ã‚Šè©°ã‚
            mel_spec = mel_spec[:MAX_FRAMES]  # åˆ‡ã‚Šè©°ã‚
            mel_spec = tf.pad(
                mel_spec, [[0, MAX_FRAMES - tf.shape(mel_spec)[0]], [0, 0]]
            )  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            mel_spec.set_shape((MAX_FRAMES, 80))
            return text_vec, mel_spec

        print("âš™ï¸  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ä¸­...")
        train_dataset = (
            dataset.unbatch()
            .filter(filter_short_audio)  # çŸ­ã™ãã‚‹éŸ³å£°ã‚’é™¤å¤–
            .map(prepare_for_training, num_parallel_calls=tf.data.AUTOTUNE)
            .cache()
            .shuffle(buffer_size=1024)
            .padded_batch(
                batch_size=32,
                padded_shapes=(
                    tf.TensorShape([None]),  # Shape for text_vec
                    tf.TensorShape([None, 80]),  # Shape for mel_spec
                ),
            )
            .prefetch(tf.data.AUTOTUNE)
        )

        # Print dataset size after preparation
        print("ğŸ“Š å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’ç¢ºèªä¸­...")
        num_after = 0
        for _ in train_dataset.unbatch():
            num_after += 1
        print(f"ğŸ“ˆ å‰å‡¦ç†å¾Œã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {num_after:,}")

        print("\n=== ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ ===")

        # ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«éŸ³å£°ã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½œæˆ
        training_plot_callback = TrainingPlotCallback(model_output_dir=model_output_dir, model_type=model_type)
        synthesis_callback = SynthesisCallback(
            text_encoder=text_encoder, n_fft=N_FFT, hop_length=HOP_LENGTH, 
            inference_text=inference_text, model_type=model_type, model_output_dir=model_output_dir
        )

        # Create checkpoint callback (save weights only)
        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=os.path.join(CHECKPOINT_DIR, "model.weights.h5"),
            save_best_only=False,
            save_weights_only=True,
            save_freq="epoch",
        )

        # Create custom callback to save training state
        class TrainingStateCallback(tf.keras.callbacks.Callback):
            def __init__(self, text_encoder, model_type, limit_samples, mode, plot_callback):
                super().__init__()
                self.text_encoder = text_encoder
                self.model_type = model_type
                self.limit_samples = limit_samples
                self.mode = mode
                self.plot_callback = plot_callback

            def on_epoch_end(self, epoch, logs=None):
                """Save training state at the end of each epoch."""
                global training_interrupted
                
                if training_interrupted:
                    print("\nâš ï¸  ä¸­æ–­ãŒè¦æ±‚ã•ã‚ŒãŸãŸã‚ã€çŠ¶æ…‹ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    return
                
                try:
                    save_training_state(epoch + 1, self.text_encoder, self.model, self.model_type, self.limit_samples, self.mode)  # Save next epoch number
                except Exception as e:
                    print(f"âŒ çŠ¶æ…‹ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç¶™ç¶šã—ã¾ã™: {e}")

            def on_batch_end(self, batch, logs=None):
                """Check for interruption during training."""
                global training_interrupted
                if training_interrupted:
                    print("\nâš ï¸  ä¸­æ–­ãŒè¦æ±‚ã•ã‚Œã¾ã—ãŸã€‚ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ã‚’å®Œäº†å¾Œã«åœæ­¢ã—ã¾ã™ã€‚")
                    self.model.stop_training = True
            
            def on_train_end(self, logs=None):
                """å­¦ç¿’çµ‚äº†æ™‚ã«å­¦ç¿’å±¥æ­´ã‚’ä¿å­˜"""
                self.plot_callback.save_training_history()

        training_state_callback = TrainingStateCallback(text_encoder, model_type, limit_samples, args.mode, training_plot_callback)

        # Update global variables before training
        global current_model, current_text_encoder, current_epoch
        current_model = model
        current_text_encoder = text_encoder
        current_epoch = start_epoch

        print(f"ğŸ¯ ã‚¨ãƒãƒƒã‚¯ {start_epoch + 1} ã‹ã‚‰ {args.epochs} ã¾ã§å­¦ç¿’ã—ã¾ã™")
        print("ğŸ’¡ Ctrl+C ã§å®‰å…¨ã«ä¸­æ–­ã§ãã¾ã™")
        print("=" * 50)
        
        # model.fitã«callbackså¼•æ•°ã‚’è¿½åŠ 
        history = model.fit(
            train_dataset,
            epochs=args.epochs,
            initial_epoch=start_epoch,
            callbacks=[
                training_plot_callback,
                synthesis_callback,
                checkpoint_callback,
                training_state_callback,
            ],
        )

        # Check if training was interrupted
        global training_interrupted
        if training_interrupted:
            print("\nâš ï¸  ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
            print("ğŸ’¾ æœ€çµ‚çŠ¶æ…‹ã‚’ä¿å­˜ä¸­...")
            try:
                save_training_state(current_epoch, text_encoder, model, model_type, limit_samples, args.mode)
                print("âœ… ä¸­æ–­æ™‚ã®çŠ¶æ…‹ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ")
            except Exception as e:
                print(f"âŒ ä¸­æ–­æ™‚ã®çŠ¶æ…‹ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return  # Early return on interruption

        print("\n=== ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸ! ===")

        # --- Save the Trained Model ---
        print("\n=== æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜ ===")
        model_save_path = os.path.join(BASE_OUTPUT_DIR, model_type.value, "ljspeech_synthesis_model.weights.h5")
        model.save_weights(model_save_path)
        print(f"ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’ä¿å­˜: {model_save_path}")

        # Save final training state
        save_training_state(args.epochs, text_encoder, model, model_type, limit_samples, args.mode)  # Final epoch

        # --- Perform Final Inference (Text-to-Speech) ---
        print("\n=== æœ€çµ‚æ¨è«–å®Ÿè¡Œ (ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°) ===")

        # æ¨è«–ã«ä½¿ç”¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        print(f"ğŸ¤ æœ€çµ‚åˆæˆç”¨ãƒ†ã‚­ã‚¹ãƒˆ: '{inference_text}'")

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        text_vec = text_encoder([inference_text])

        # ãƒ¢ãƒ‡ãƒ«ã§ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’äºˆæ¸¬
        model_output = model.predict(text_vec)
        
        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦é©åˆ‡ãªå‡ºåŠ›ã‚’å–å¾—
        if isinstance(model_output, dict):
            if model_type == TTSModel.FASTSPEECH2:
                predicted_mel_spec = model_output['mel_output_refined']
            else:
                # ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®å ´åˆã€åˆ©ç”¨å¯èƒ½ãªå‡ºåŠ›ã‚­ãƒ¼ã‹ã‚‰é¸æŠ
                predicted_mel_spec = model_output.get('mel_output_refined', 
                                                    model_output.get('mel_output',
                                                                    list(model_output.values())[0]))
        else:
            predicted_mel_spec = model_output

        # ãƒãƒƒãƒæ¬¡å…ƒã‚’å‰Šé™¤ã—ã€numpyé…åˆ—ã«å¤‰æ›
        predicted_mel_spec_np = predicted_mel_spec[0]

        # ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’ãƒ‡ã‚·ãƒ™ãƒ«ã‹ã‚‰ãƒ‘ãƒ¯ãƒ¼ã«å¤‰æ›
        predicted_mel_spec_db_t = predicted_mel_spec_np.T
        power_spec = librosa.db_to_power(predicted_mel_spec_db_t)

        # Griffin-Limã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§éŸ³å£°ã‚’å¾©å…ƒ
        print("ğŸµ Griffin-Limã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§éŸ³å£°ã‚’åˆæˆä¸­...")
        generated_audio = librosa.feature.inverse.mel_to_audio(
            power_spec, sr=22050, n_fft=N_FFT, hop_length=HOP_LENGTH
        )

        # ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ã‚’ä¿å­˜
        output_audio_path = os.path.join(BASE_OUTPUT_DIR, model_type.value, "synthesized_audio_final.wav")
        sf.write(output_audio_path, generated_audio, 22050)
        print(f"ğŸµ æœ€çµ‚åˆæˆéŸ³å£°ã‚’ä¿å­˜: {output_audio_path}")

        # ç”Ÿæˆã•ã‚ŒãŸã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã¨éŸ³å£°ã‚’å¯è¦–åŒ–
        visualize_audio_and_spectrogram(
            tf.convert_to_tensor(generated_audio),
            inference_text,
            save_path=os.path.join(BASE_OUTPUT_DIR, model_type.value, "synthesis_visualization_final.png"),
        )

        print(f"\nğŸ• å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("\nâœ… ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        signal_handler(signal.SIGINT, None)
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        
        # Try to save current state if possible
        try:
            if 'current_model' in globals() and current_model is not None:
                print("ğŸ†˜ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®ç·Šæ€¥çŠ¶æ…‹ä¿å­˜ã‚’è©¦è¡Œä¸­...")
                # Use default values for emergency save if variables are not available
                emergency_limit_samples = locals().get('limit_samples', 10)
                emergency_mode = locals().get('args', type('', (), {'mode': 'mini'})).mode
                save_training_state(current_epoch, current_text_encoder, current_model, model_type, emergency_limit_samples, emergency_mode)
                print("âœ… ç·Šæ€¥çŠ¶æ…‹ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ")
        except Exception as save_error:
            print(f"âŒ ç·Šæ€¥çŠ¶æ…‹ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {save_error}")


if __name__ == "__main__":
    main()
