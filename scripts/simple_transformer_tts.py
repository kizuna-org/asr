#!/usr/bin/env python3
"""
Encoder-Decoder Transformer TTS Model Implementation
This script implements a Transformer-based TTS model with encoder-decoder architecture.
"""

import tensorflow as tf
import numpy as np
from typing import Tuple, Optional, Dict, Any
import math


class PositionalEncoding(tf.keras.layers.Layer):
    """ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å±¤ - Transformerã«ç³»åˆ—ã®é †åºæƒ…å ±ã‚’æä¾›"""
    
    def __init__(self, d_model: int, max_seq_length: int = 5000, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model
        self.max_seq_length = max_seq_length
        
        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’äº‹å‰è¨ˆç®—
        position = np.arange(max_seq_length)[:, np.newaxis]
        div_term = np.exp(np.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        
        pe = np.zeros((max_seq_length, d_model))
        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)
        
        self.positional_encoding = tf.constant(pe, dtype=tf.float32)
    
    def call(self, inputs):
        seq_len = tf.shape(inputs)[1]
        return inputs + self.positional_encoding[:seq_len]
    
    def get_config(self):
        config = super().get_config()
        config.update({
            'd_model': self.d_model,
            'max_seq_length': self.max_seq_length
        })
        return config


class NoamLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    """Noamå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© - Attention Is All You Needã§ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•"""
    
    def __init__(self, d_model: int, warmup_steps: int = 4000, **kwargs):
        super().__init__(**kwargs)
        self.d_model = tf.cast(d_model, tf.float32)
        self.warmup_steps = tf.cast(warmup_steps, tf.float32)
    
    def __call__(self, step):
        step = tf.cast(step, tf.float32)
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)
        return tf.math.rsqrt(self.d_model) * tf.minimum(arg1, arg2)
    
    def get_config(self):
        return {
            'd_model': int(self.d_model),
            'warmup_steps': int(self.warmup_steps)
        }


class TransformerEncoderLayer(tf.keras.layers.Layer):
    """Transformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å±¤"""
    
    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout_rate: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.dropout_rate = dropout_rate
        
        # Multi-Head Self-Attention
        self.self_attention = tf.keras.layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=d_model // num_heads,
            dropout=dropout_rate
        )
        
        # Feed-Forward Network
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(d_ff, activation='relu'),
            tf.keras.layers.Dense(d_model),
            tf.keras.layers.Dropout(dropout_rate)
        ])
        
        # Layer Normalization
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        
        # Dropout
        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)
        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)
    
    def call(self, x, training=None, mask=None):
        # Self-Attention
        attn_output = self.self_attention(x, x, attention_mask=mask, training=training)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)
        
        # Feed-Forward Network
        ffn_output = self.ffn(out1, training=training)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)
        
        return out2
    
    def get_config(self):
        config = super().get_config()
        config.update({
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'd_ff': self.d_ff,
            'dropout_rate': self.dropout_rate
        })
        return config


class TransformerDecoderLayer(tf.keras.layers.Layer):
    """Transformerãƒ‡ã‚³ãƒ¼ãƒ€å±¤"""
    
    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout_rate: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.dropout_rate = dropout_rate
        
        # Masked Multi-Head Self-Attention
        self.self_attention = tf.keras.layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=d_model // num_heads,
            dropout=dropout_rate
        )
        
        # Multi-Head Cross-Attention (Encoder-Decoder Attention)
        self.cross_attention = tf.keras.layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=d_model // num_heads,
            dropout=dropout_rate
        )
        
        # Feed-Forward Network
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(d_ff, activation='relu'),
            tf.keras.layers.Dense(d_model),
            tf.keras.layers.Dropout(dropout_rate)
        ])
        
        # Layer Normalization
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        
        # Dropout
        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)
        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)
        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)
    
    def call(self, x, encoder_output, training=None, look_ahead_mask=None, padding_mask=None):
        # Masked Self-Attention
        attn1 = self.self_attention(x, x, attention_mask=look_ahead_mask, training=training)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(attn1 + x)
        
        # Cross-Attention
        attn2 = self.cross_attention(out1, encoder_output, attention_mask=padding_mask, training=training)
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(attn2 + out1)
        
        # Feed-Forward Network
        ffn_output = self.ffn(out2, training=training)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(ffn_output + out2)
        
        return out3
    
    def get_config(self):
        config = super().get_config()
        config.update({
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'd_ff': self.d_ff,
            'dropout_rate': self.dropout_rate
        })
        return config


class Prenet(tf.keras.layers.Layer):
    """ãƒ‡ã‚³ãƒ¼ãƒ€ã¸ã®å…¥åŠ›å“è³ªã‚’é«˜ã‚ã‚‹ãŸã‚ã®å°ã•ãªå…¨çµåˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    
    def __init__(self, units: int = 256, dropout_rate: float = 0.5, **kwargs):
        super().__init__(**kwargs)
        self.units = units
        self.dropout_rate = dropout_rate
        
        self.dense1 = tf.keras.layers.Dense(units, activation='relu')
        self.dense2 = tf.keras.layers.Dense(units, activation='relu')
        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)
        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)
    
    def call(self, x, training=None):
        x = self.dense1(x)
        x = self.dropout1(x, training=training)
        x = self.dense2(x)
        x = self.dropout2(x, training=training)
        return x
    
    def get_config(self):
        config = super().get_config()
        config.update({
            'units': self.units,
            'dropout_rate': self.dropout_rate
        })
        return config


class SimpleTransformerTTS(tf.keras.Model):
    """Encoder-Decoder Transformer TTS model."""
    
    def __init__(self, config: Dict[str, Any], **kwargs):
        super().__init__(**kwargs)
        self.config = config
        
        # Model parameters
        self.vocab_size = config['vocab_size']
        self.d_model = config['d_model']
        self.encoder_layers = config['encoder_layers']
        self.decoder_layers = config['decoder_layers']
        self.num_heads = config['num_heads']
        self.d_ff = config['d_ff']
        self.n_mels = config['n_mels']
        self.dropout_rate = config.get('dropout_rate', 0.1)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (Text Encoder)
        self.text_embedding = tf.keras.layers.Embedding(self.vocab_size, self.d_model)
        self.encoder_pos_encoding = PositionalEncoding(self.d_model)
        
        self.encoder_layers_list = [
            TransformerEncoderLayer(self.d_model, self.num_heads, self.d_ff, self.dropout_rate)
            for _ in range(self.encoder_layers)
        ]
        
        # ãƒ¡ãƒ«ãƒ‡ã‚³ãƒ¼ãƒ€ (Mel Decoder)
        self.prenet = Prenet(units=256, dropout_rate=0.5)
        self.decoder_projection = tf.keras.layers.Dense(self.d_model)
        self.decoder_pos_encoding = PositionalEncoding(self.d_model)
        
        self.decoder_layers_list = [
            TransformerDecoderLayer(self.d_model, self.num_heads, self.d_ff, self.dropout_rate)
            for _ in range(self.decoder_layers)
        ]
        
        # æœ€çµ‚å‡ºåŠ›éƒ¨ (Final Output Stage)
        # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ç”Ÿæˆ
        self.mel_linear = tf.keras.layers.Dense(self.n_mels)
        
        # Stop Tokenäºˆæ¸¬
        self.stop_linear = tf.keras.layers.Dense(1, activation='sigmoid')
        
        # Post-netï¼ˆ5å±¤ã®ç•³ã¿è¾¼ã¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰
        self.postnet = tf.keras.Sequential([
            # 1å±¤ç›®
            tf.keras.layers.Conv1D(512, 5, padding='same', activation='tanh'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dropout(self.dropout_rate),
            
            # 2å±¤ç›®
            tf.keras.layers.Conv1D(512, 5, padding='same', activation='tanh'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dropout(self.dropout_rate),
            
            # 3å±¤ç›®
            tf.keras.layers.Conv1D(512, 5, padding='same', activation='tanh'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dropout(self.dropout_rate),
            
            # 4å±¤ç›®
            tf.keras.layers.Conv1D(512, 5, padding='same', activation='tanh'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dropout(self.dropout_rate),
            
            # 5å±¤ç›®ï¼ˆå‡ºåŠ›å±¤ã€æ´»æ€§åŒ–é–¢æ•°ãªã—ï¼‰
            tf.keras.layers.Conv1D(self.n_mels, 5, padding='same')
        ])
    
    def create_look_ahead_mask(self, size):
        """æœªæ¥ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚«ãƒ³ãƒ‹ãƒ³ã‚°ã—ãªã„ã‚ˆã†ã«ãƒã‚¹ã‚¯ã‚’ä½œæˆ"""
        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
        return mask  # (seq_len, seq_len)
    
    def encode(self, text_inputs, training=None):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€"""
        # ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        x = self.text_embedding(text_inputs)
        x = self.encoder_pos_encoding(x)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å±¤ã‚’é€šã™
        for encoder_layer in self.encoder_layers_list:
            x = encoder_layer(x, training=training)
        
        return x
    
    def decode_step(self, mel_inputs, encoder_output, training=None):
        """ãƒ¡ãƒ«ãƒ‡ã‚³ãƒ¼ãƒ€ã®1ã‚¹ãƒ†ãƒƒãƒ—"""
        # Prenet
        x = self.prenet(mel_inputs, training=training)
        x = self.decoder_projection(x)
        x = self.decoder_pos_encoding(x)
        
        # Look-ahead maskä½œæˆ
        seq_len = tf.shape(x)[1]
        look_ahead_mask = self.create_look_ahead_mask(seq_len)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€å±¤ã‚’é€šã™
        for decoder_layer in self.decoder_layers_list:
            x = decoder_layer(
                x, encoder_output,
                training=training,
                look_ahead_mask=look_ahead_mask
            )
        
        return x
    
    def get_config(self):
        """Return the config of the model for serialization."""
        base_config = super().get_config()
        return {**base_config, 'config': self.config}
    
    @classmethod
    def from_config(cls, config):
        """Create a model from its config."""
        # Extract the model-specific config
        model_config = config.pop('config', {})
        return cls(model_config, **config)
    
    def call(self, inputs: tf.Tensor, training: Optional[bool] = None, **kwargs) -> Dict[str, tf.Tensor]:
        """
        Forward pass for training mode (teacher forcing)
        inputs: text inputs (batch_size, text_seq_len)
        """
        # For training, we need mel targets for teacher forcing
        # This is a simplified implementation that assumes mel targets are provided
        # In practice, you would need to modify this based on your training setup
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
        encoder_output = self.encode(inputs, training=training)
        
        # ç°¡ç•¥åŒ–ã®ãŸã‚ã€å›ºå®šé•·ã®ãƒ¡ãƒ«å‡ºåŠ›ã‚’ç”Ÿæˆ
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€è‡ªå·±å›å¸°çš„ã«ç”Ÿæˆã™ã‚‹ã‹ã€teacher forcingã‚’ä½¿ç”¨
        batch_size = tf.shape(inputs)[0]
        max_mel_length = 100  # é©åˆ‡ãªé•·ã•ã«èª¿æ•´
        
        # ãƒ€ãƒŸãƒ¼ã®ãƒ¡ãƒ«å…¥åŠ›ï¼ˆå®Ÿéš›ã¯GTãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã¾ãŸã¯å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ï¼‰
        dummy_mel = tf.zeros((batch_size, max_mel_length, self.n_mels))
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€
        decoder_output = self.decode_step(dummy_mel, encoder_output, training=training)
        
        # æœ€çµ‚å‡ºåŠ›
        mel_output = self.mel_linear(decoder_output)
        stop_tokens = self.stop_linear(decoder_output)
        
        # Post-netã§è£œæ­£
        mel_postnet = self.postnet(mel_output, training=training)
        mel_output_refined = mel_output + mel_postnet
        
        return {
            'encoder_output': encoder_output,
            'decoder_output': decoder_output,
            'mel_output': mel_output,
            'mel_output_refined': mel_output_refined,
            'stop_tokens': stop_tokens,
        }


def create_simple_transformer_config() -> Dict[str, Any]:
    """Create default configuration for Encoder-Decoder Transformer TTS model."""
    return {
        'vocab_size': 10000,
        'd_model': 512,
        'encoder_layers': 6,
        'decoder_layers': 6,
        'num_heads': 8,
        'd_ff': 2048,  # d_modelã®4å€
        'n_mels': 80,
        'dropout_rate': 0.1,
        'learning_rate': 1e-4,
        'warmup_steps': 4000,
    }


def build_simple_transformer_tts_model(vocab_size: int = 10000, 
                                      n_mels: int = 80,
                                      config: Optional[Dict[str, Any]] = None) -> 'TTSModelTrainer':
    """Build and return a Encoder-Decoder Transformer TTS model wrapped in TTSModelTrainer."""
    # Import here to avoid circular imports
    from ljspeech_demo import TTSModelTrainer, TTSModel
    
    if config is None:
        config = create_simple_transformer_config()
    
    config['vocab_size'] = vocab_size
    config['n_mels'] = n_mels
    
    # Create the base model
    simple_model = SimpleTransformerTTS(config)
    
    # Build the model by calling it with dummy data
    dummy_text = tf.zeros((1, 10), dtype=tf.int32)
    _ = simple_model(dummy_text, training=False)
    
    # Wrap with trainer class
    model = TTSModelTrainer(simple_model, TTSModel.TRANSFORMER_TTS)
    
    # Build the wrapper model too
    _ = model(dummy_text, training=False)
    
    # Noamå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä½¿ç”¨ã—ãŸOptimizerã‚’ä½œæˆ
    learning_rate_schedule = NoamLearningRateSchedule(
        d_model=config['d_model'],
        warmup_steps=config.get('warmup_steps', 4000)
    )
    
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=learning_rate_schedule,
        beta_1=0.9,
        beta_2=0.98,
        epsilon=1e-9
    )
    
    # Compile the model
    model.compile(
        optimizer=optimizer,
        metrics=['mae']
    )
    
    print(f"âœ… Encoder-Decoder Transformer TTS model created successfully")
    print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€: {config['encoder_layers']}å±¤")
    print(f"ğŸ¯ ãƒ¡ãƒ«ãƒ‡ã‚³ãƒ¼ãƒ€: {config['decoder_layers']}å±¤")
    print(f"ğŸ”§ Multi-Head Attention: {config['num_heads']}ãƒ˜ãƒƒãƒ‰")
    print(f"ğŸ“ˆ FFNå†…éƒ¨æ¬¡å…ƒ: {config['d_ff']} (d_modelã®4å€)")
    print(f"ğŸµ Prenet: 256ãƒ¦ãƒ‹ãƒƒãƒˆ (ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ0.5)")
    print(f"ğŸ Stop Tokenäºˆæ¸¬: æœ‰åŠ¹")
    print(f"ğŸ“ˆ Noamå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©: æœ‰åŠ¹ (ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—: {config.get('warmup_steps', 4000)})")
    print(f"ğŸ”§ Post-net: 5å±¤ç•³ã¿è¾¼ã¿æ§‹é€ ")
    
    # Try to get parameter count with error handling
    try:
        param_count = model.count_params()
        print(f"Model parameters: {param_count:,}")
    except ValueError as e:
        print(f"Model parameters: Could not count parameters - {e}")
    
    return model


class SimpleTransformerTTSLoss(tf.keras.losses.Loss):
    """Loss function for Encoder-Decoder Transformer TTS model."""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.mse = tf.keras.losses.MeanSquaredError()
        self.bce = tf.keras.losses.BinaryCrossentropy()
    
    def call(self, y_true, y_pred):
        """
        Compute loss for Encoder-Decoder Transformer TTS model.
        
        y_true: dict containing mel-spectrogram and stop tokens
        y_pred: dict containing model outputs
        """
        # Mel-spectrogram loss (both before and after postnet)
        mel_loss = self.mse(y_true['mel'], y_pred['mel_output'])
        mel_postnet_loss = self.mse(y_true['mel'], y_pred['mel_output_refined'])
        
        # Stop token loss
        stop_loss = self.bce(y_true['stop_tokens'], y_pred['stop_tokens'])
        
        total_loss = mel_loss + mel_postnet_loss + stop_loss
        
        return total_loss


if __name__ == "__main__":
    # Test the model
    config = create_simple_transformer_config()
    model = build_simple_transformer_tts_model(config=config)
    
    # Test with dummy input
    dummy_text = tf.random.uniform((2, 20), 0, config['vocab_size'], dtype=tf.int32)
    output = model(dummy_text, training=False)
    
    print("Model output shapes:")
    for key, value in output.items():
        print(f"  {key}: {value.shape}")
