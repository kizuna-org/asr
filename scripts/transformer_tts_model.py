#!/usr/bin/env python3
"""
Transformer-based TTS Model Implementation
This script implements modern transformer-based TTS models including VITS and vanilla Transformer TTS.
GPU-optimized implementation for high-performance training and inference.
"""

import tensorflow as tf
import numpy as np
from typing import Tuple, Optional, Dict, Any
import math
import os


def setup_gpu():
    """Setup GPU configuration for TensorFlow."""
    print("üîß Configuring GPU settings...")
    
    # Check GPU availability
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # Enable memory growth to avoid allocating all GPU memory at once
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            
            print(f"‚úÖ Found {len(gpus)} GPU(s): {[gpu.name for gpu in gpus]}")
            
            # Set mixed precision policy for better GPU performance
            policy = tf.keras.mixed_precision.Policy('mixed_float16')
            tf.keras.mixed_precision.set_global_policy(policy)
            print("‚úÖ Mixed precision (float16) enabled for better GPU performance")
            
            # Optional: Set specific GPU device
            logical_gpus = tf.config.experimental.list_logical_devices('GPU')
            print(f"‚úÖ Logical GPUs: {len(logical_gpus)}")
            
        except RuntimeError as e:
            print(f"‚ùå GPU configuration error: {e}")
    else:
        print("‚ö†Ô∏è  No GPU found. Running on CPU.")
    
    return len(gpus) > 0


def get_gpu_strategy():
    """Get distributed training strategy for multi-GPU setup."""
    gpus = tf.config.experimental.list_physical_devices('GPU')
    
    if len(gpus) > 1:
        print(f"üöÄ Setting up multi-GPU strategy with {len(gpus)} GPUs")
        strategy = tf.distribute.MirroredStrategy()
        print(f"‚úÖ Multi-GPU strategy initialized with {strategy.num_replicas_in_sync} replicas")
        return strategy
    elif len(gpus) == 1:
        print("üöÄ Single GPU strategy")
        strategy = tf.distribute.OneDeviceStrategy(device="/gpu:0")
        return strategy
    else:
        print("‚ö†Ô∏è  No GPU available, using CPU strategy")
        return tf.distribute.get_strategy()  # Default strategy


class MultiHeadAttention(tf.keras.layers.Layer):
    """Multi-Head Attention with GPU-optimized efficiency."""
    
    def __init__(self, d_model: int, num_heads: int, dropout_rate: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model
        self.num_heads = num_heads
        self.dropout_rate = dropout_rate
        
        assert d_model % num_heads == 0
        self.depth = d_model // num_heads
        
        # GPU-optimized dense layers with float16 compatibility
        self.wq = tf.keras.layers.Dense(d_model, use_bias=False, dtype='float32')
        self.wk = tf.keras.layers.Dense(d_model, use_bias=False, dtype='float32')
        self.wv = tf.keras.layers.Dense(d_model, use_bias=False, dtype='float32')
        self.dense = tf.keras.layers.Dense(d_model, dtype='float32')
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        
    def split_heads(self, x: tf.Tensor, batch_size: int) -> tf.Tensor:
        """Split the last dimension into (num_heads, depth)."""
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    def scaled_dot_product_attention(self, q: tf.Tensor, k: tf.Tensor, v: tf.Tensor, 
                                   mask: Optional[tf.Tensor] = None) -> Tuple[tf.Tensor, tf.Tensor]:
        """Calculate attention weights with GPU-optimized numerical stability."""
        # Use efficient GPU matrix multiplication
        with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):
            matmul_qk = tf.matmul(q, k, transpose_b=True)
            
            # Scale with stable computation
            dk = tf.cast(tf.shape(k)[-1], tf.float32)
            scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
            
            # Apply mask with GPU-optimized operations
            if mask is not None:
                scaled_attention_logits += (mask * -1e9)
            
            # GPU-optimized softmax
            attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
            attention_weights = self.dropout(attention_weights)
            
            # Final matrix multiplication
            output = tf.matmul(attention_weights, v)
            
        return output, attention_weights
    
    def call(self, q: tf.Tensor, k: tf.Tensor, v: tf.Tensor, 
             mask: Optional[tf.Tensor] = None, training: Optional[bool] = None) -> tf.Tensor:
        batch_size = tf.shape(q)[0]
        
        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)
        
        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)
        
        scaled_attention, attention_weights = self.scaled_dot_product_attention(
            q, k, v, mask)
        
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(scaled_attention, 
                                    (batch_size, -1, self.d_model))
        
        output = self.dense(concat_attention)
        return output


class FeedForward(tf.keras.layers.Layer):
    """Feed Forward Network with GLU activation."""
    
    def __init__(self, d_model: int, d_ff: int, dropout_rate: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model
        self.d_ff = d_ff
        self.dropout_rate = dropout_rate
        
        self.linear1 = tf.keras.layers.Dense(d_ff * 2)  # For GLU
        self.linear2 = tf.keras.layers.Dense(d_model)
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        
    def call(self, x: tf.Tensor, training: Optional[bool] = None) -> tf.Tensor:
        # GLU activation
        x = self.linear1(x)
        x1, x2 = tf.split(x, 2, axis=-1)
        x = x1 * tf.nn.sigmoid(x2)
        x = self.dropout(x, training=training)
        x = self.linear2(x)
        return x


class TransformerLayer(tf.keras.layers.Layer):
    """Transformer layer with pre-norm architecture."""
    
    def __init__(self, d_model: int, num_heads: int, d_ff: int, 
                 dropout_rate: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.dropout_rate = dropout_rate
        
        self.mha = MultiHeadAttention(d_model, num_heads, dropout_rate)
        self.ffn = FeedForward(d_model, d_ff, dropout_rate)
        
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)
        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)
    
    def call(self, x: tf.Tensor, mask: Optional[tf.Tensor] = None,
             training: Optional[bool] = None) -> tf.Tensor:
        # Pre-norm architecture
        attn_input = self.layernorm1(x)
        attn_output = self.mha(attn_input, attn_input, attn_input, mask=mask, training=training)
        attn_output = self.dropout1(attn_output, training=training)
        x = x + attn_output
        
        ffn_input = self.layernorm2(x)
        ffn_output = self.ffn(ffn_input, training=training)
        ffn_output = self.dropout2(ffn_output, training=training)
        x = x + ffn_output
        
        return x


class PositionalEncoding(tf.keras.layers.Layer):
    """Sinusoidal positional encoding."""
    
    def __init__(self, max_length: int = 5000, **kwargs):
        super().__init__(**kwargs)
        self.max_length = max_length
        
    def build(self, input_shape):
        d_model = input_shape[-1]
        self.d_model = d_model
        pos_encoding = self.positional_encoding(self.max_length, d_model)
        self.pos_encoding = self.add_weight(
            name='pos_encoding',
            shape=pos_encoding.shape,
            initializer='zeros',
            trainable=False
        )
        self.pos_encoding.assign(pos_encoding)
        
    def positional_encoding(self, position: int, d_model: int) -> tf.Tensor:
        """Generate sinusoidal positional encoding."""
        def get_angles(pos, i, d_model):
            angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))
            return pos * angle_rates
        
        angle_rads = get_angles(np.arange(position)[:, np.newaxis],
                               np.arange(d_model)[np.newaxis, :],
                               d_model)
        
        # Apply sin to even indices
        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
        
        # Apply cos to odd indices
        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
        
        pos_encoding = angle_rads[np.newaxis, ...]
        return tf.cast(pos_encoding, dtype=tf.float32)
    
    def call(self, x: tf.Tensor) -> tf.Tensor:
        seq_len = tf.shape(x)[1]
        return x + self.pos_encoding[:, :seq_len, :]


class DurationPredictor(tf.keras.layers.Layer):
    """Duration predictor for non-autoregressive TTS."""
    
    def __init__(self, d_model: int, n_layers: int = 2, 
                 kernel_size: int = 3, dropout_rate: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model
        self.n_layers = n_layers
        self.kernel_size = kernel_size
        self.dropout_rate = dropout_rate
        
        self.conv_layers = []
        for i in range(n_layers):
            self.conv_layers.append(tf.keras.layers.Conv1D(
                filters=d_model,
                kernel_size=kernel_size,
                padding='same',
                activation='relu'
            ))
            self.conv_layers.append(tf.keras.layers.LayerNormalization())
            self.conv_layers.append(tf.keras.layers.Dropout(dropout_rate))
        
        self.linear = tf.keras.layers.Dense(1)
        
    def call(self, x: tf.Tensor, training: Optional[bool] = None) -> tf.Tensor:
        for layer in self.conv_layers:
            if isinstance(layer, tf.keras.layers.Dropout):
                x = layer(x, training=training)
            else:
                x = layer(x)
        
        duration = self.linear(x)
        duration = tf.nn.softplus(duration)  # Ensure positive values
        return duration


class VariancePredictor(tf.keras.layers.Layer):
    """Variance predictor for pitch and energy."""
    
    def __init__(self, d_model: int, n_layers: int = 2, 
                 kernel_size: int = 3, dropout_rate: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model
        self.n_layers = n_layers
        self.kernel_size = kernel_size
        self.dropout_rate = dropout_rate
        
        self.conv_layers = []
        for i in range(n_layers):
            self.conv_layers.append(tf.keras.layers.Conv1D(
                filters=d_model,
                kernel_size=kernel_size,
                padding='same',
                activation='relu'
            ))
            self.conv_layers.append(tf.keras.layers.LayerNormalization())
            self.conv_layers.append(tf.keras.layers.Dropout(dropout_rate))
        
        self.linear = tf.keras.layers.Dense(1)
        
    def call(self, x: tf.Tensor, training: Optional[bool] = None) -> tf.Tensor:
        for layer in self.conv_layers:
            if isinstance(layer, tf.keras.layers.Dropout):
                x = layer(x, training=training)
            else:
                x = layer(x)
        
        variance = self.linear(x)
        return variance


class LengthRegulator(tf.keras.layers.Layer):
    """Length regulator for duration-based upsampling."""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
    def call(self, x: tf.Tensor, duration: tf.Tensor) -> tf.Tensor:
        """Regulate length based on predicted duration."""
        # Simplified implementation: just return the input with some expansion
        # In practice, this would use the duration to expand the sequence
        batch_size = tf.shape(x)[0]
        seq_len = tf.shape(x)[1]
        d_model = tf.shape(x)[2]
        
        # Simple expansion factor (in real implementation, use duration)
        expansion_factor = 2
        expanded_seq_len = seq_len * expansion_factor
        
        # Repeat each frame according to expansion factor
        x_expanded = tf.repeat(x, expansion_factor, axis=1)
        
        return x_expanded


class TransformerTTS(tf.keras.Model):
    """Modern Transformer-based TTS model."""
    
    def __init__(self, config: Dict[str, Any], **kwargs):
        super().__init__(**kwargs)
        self.config = config
        
        # Model parameters
        self.vocab_size = config['vocab_size']
        self.d_model = config['d_model']
        self.num_encoder_layers = config['num_encoder_layers']
        self.num_decoder_layers = config['num_decoder_layers']
        self.num_heads = config['num_heads']
        self.d_ff = config['d_ff']
        self.max_seq_len = config['max_seq_len']
        self.n_mels = config['n_mels']
        self.dropout_rate = config.get('dropout_rate', 0.1)
        
        # Embedding layers
        self.text_embedding = tf.keras.layers.Embedding(self.vocab_size, self.d_model)
        self.positional_encoding = PositionalEncoding(self.max_seq_len)
        
        # Encoder
        self.encoder_layers = [
            TransformerLayer(self.d_model, self.num_heads, self.d_ff, self.dropout_rate)
            for _ in range(self.num_encoder_layers)
        ]
        self.encoder_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        
        # Variance predictors
        self.duration_predictor = DurationPredictor(self.d_model, dropout_rate=self.dropout_rate)
        self.pitch_predictor = VariancePredictor(self.d_model, dropout_rate=self.dropout_rate)
        self.energy_predictor = VariancePredictor(self.d_model, dropout_rate=self.dropout_rate)
        
        # Length regulator
        self.length_regulator = LengthRegulator()
        
        # Pitch and energy embeddings
        self.pitch_embedding = tf.keras.layers.Dense(self.d_model)
        self.energy_embedding = tf.keras.layers.Dense(self.d_model)
        
        # Decoder
        self.decoder_layers = [
            TransformerLayer(self.d_model, self.num_heads, self.d_ff, self.dropout_rate)
            for _ in range(self.num_decoder_layers)
        ]
        self.decoder_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        
        # Output projection
        self.mel_linear = tf.keras.layers.Dense(self.n_mels)
        
        # Postnet
        self.postnet = self._build_postnet()
        
    def _build_postnet(self):
        """Build postnet for mel-spectrogram refinement."""
        layers = []
        n_convs = 5
        postnet_dim = 512
        kernel_size = 5
        
        for i in range(n_convs):
            output_dim = self.n_mels if i == n_convs - 1 else postnet_dim
            activation = None if i == n_convs - 1 else 'tanh'
            
            layers.append(tf.keras.layers.Conv1D(
                filters=output_dim,
                kernel_size=kernel_size,
                padding='same',
                activation=activation
            ))
            layers.append(tf.keras.layers.BatchNormalization())
            if i < n_convs - 1:
                layers.append(tf.keras.layers.Dropout(self.dropout_rate))
        
        return tf.keras.Sequential(layers)
    
    def call(self, inputs: tf.Tensor, duration: Optional[tf.Tensor] = None,
             pitch: Optional[tf.Tensor] = None, energy: Optional[tf.Tensor] = None,
             training: Optional[bool] = None) -> Dict[str, tf.Tensor]:
        
        # Text encoding
        x = self.text_embedding(inputs)
        x = self.positional_encoding(x)
        
        # Encoder
        for layer in self.encoder_layers:
            x = layer(x, training=training)
        encoder_output = self.encoder_norm(x)
        
        # Variance prediction
        duration_pred = self.duration_predictor(encoder_output, training=training)
        pitch_pred = self.pitch_predictor(encoder_output, training=training)
        energy_pred = self.energy_predictor(encoder_output, training=training)
        
        # Use predicted or ground truth variance
        duration_used = duration if duration is not None else duration_pred
        pitch_used = pitch if pitch is not None else pitch_pred
        energy_used = energy if energy is not None else energy_pred
        
        # Length regulation
        regulated_output = self.length_regulator(encoder_output, duration_used)
        
        # Add variance information
        pitch_emb = self.pitch_embedding(pitch_used)
        energy_emb = self.energy_embedding(energy_used)
        
        # Ensure shapes match for addition
        regulated_seq_len = tf.shape(regulated_output)[1]
        pitch_emb = pitch_emb[:, :regulated_seq_len, :]
        energy_emb = energy_emb[:, :regulated_seq_len, :]
        
        decoder_input = regulated_output + pitch_emb + energy_emb
        
        # Decoder
        for layer in self.decoder_layers:
            decoder_input = layer(decoder_input, training=training)
        decoder_output = self.decoder_norm(decoder_input)
        
        # Mel-spectrogram prediction
        mel_output = self.mel_linear(decoder_output)
        
        # Postnet refinement
        mel_postnet = self.postnet(mel_output, training=training)
        mel_output_refined = mel_output + mel_postnet
        
        return {
            'mel_output': mel_output,
            'mel_output_refined': mel_output_refined,
            'duration_pred': duration_pred,
            'pitch_pred': pitch_pred,
            'energy_pred': energy_pred,
            'encoder_output': encoder_output
        }


def create_transformer_tts_config() -> Dict[str, Any]:
    """Create default configuration for Transformer TTS model."""
    return {
        'vocab_size': 10000,
        'd_model': 512,
        'num_encoder_layers': 6,
        'num_decoder_layers': 6,
        'num_heads': 8,
        'd_ff': 2048,
        'max_seq_len': 1000,
        'n_mels': 80,
        'dropout_rate': 0.1,
        'learning_rate': 1e-4,
        'warmup_steps': 4000,
    }


def build_transformer_tts_model(vocab_size: int = 10000, 
                               n_mels: int = 80,
                               config: Optional[Dict[str, Any]] = None,
                               use_gpu: bool = True) -> TransformerTTS:
    """Build and return a GPU-optimized Transformer TTS model."""
    if config is None:
        config = create_transformer_tts_config()
    
    config['vocab_size'] = vocab_size
    config['n_mels'] = n_mels
    
    # Setup GPU if requested and available
    gpu_available = False
    if use_gpu:
        gpu_available = setup_gpu()
    
    # Get appropriate strategy for training
    strategy = get_gpu_strategy()
    
    with strategy.scope():
        model = TransformerTTS(config)
        
        # Build the model by calling it with dummy data on appropriate device
        device = '/GPU:0' if gpu_available else '/CPU:0'
        with tf.device(device):
            dummy_text = tf.zeros((1, 10), dtype=tf.int32)
            _ = model(dummy_text, training=False)
    
    print(f"‚úÖ Transformer TTS model created successfully on {device}")
    print(f"üìä Model parameters: {model.count_params():,}")
    print(f"üéØ Mixed precision: {tf.keras.mixed_precision.global_policy().name}")
    
    return model


class TransformerTTSLoss(tf.keras.losses.Loss):
    """GPU-optimized custom loss function for Transformer TTS model."""
    
    def __init__(self, mel_loss_weight=1.0, duration_loss_weight=0.1, 
                 pitch_loss_weight=0.1, energy_loss_weight=0.1, **kwargs):
        super().__init__(**kwargs)
        self.mel_loss_weight = mel_loss_weight
        self.duration_loss_weight = duration_loss_weight
        self.pitch_loss_weight = pitch_loss_weight
        self.energy_loss_weight = energy_loss_weight
        
        # GPU-optimized loss functions with mixed precision support
        self.mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)
        self.mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
    
    def call(self, y_true, y_pred):
        """
        Compute GPU-optimized loss for Transformer TTS model.
        
        y_true: mel-spectrogram (batch_size, time_steps, n_mels)
        y_pred: dict containing model outputs
        """
        # Ensure computations happen on GPU if available
        with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):
            # Mel-spectrogram loss (both before and after postnet)
            mel_loss = tf.reduce_mean(self.mse(y_true, y_pred['mel_output']))
            mel_postnet_loss = tf.reduce_mean(self.mse(y_true, y_pred['mel_output_refined']))
            total_mel_loss = mel_loss + mel_postnet_loss
            
            # Variance losses (simplified targets for now)
            batch_size = tf.shape(y_true)[0]
            seq_length = tf.shape(y_pred['duration_pred'])[1]
            
            # In real implementation, these would be extracted from data
            duration_target = tf.ones((batch_size, seq_length, 1), dtype=tf.float32)
            pitch_target = tf.zeros((batch_size, seq_length, 1), dtype=tf.float32)
            energy_target = tf.zeros((batch_size, seq_length, 1), dtype=tf.float32)
            
            duration_loss = tf.reduce_mean(self.mae(duration_target, y_pred['duration_pred']))
            pitch_loss = tf.reduce_mean(self.mse(pitch_target, y_pred['pitch_pred']))
            energy_loss = tf.reduce_mean(self.mse(energy_target, y_pred['energy_pred']))
            
            total_loss = (self.mel_loss_weight * total_mel_loss +
                         self.duration_loss_weight * duration_loss +
                         self.pitch_loss_weight * pitch_loss +
                         self.energy_loss_weight * energy_loss)
            
            # Cast to float32 for mixed precision compatibility
            total_loss = tf.cast(total_loss, tf.float32)
            
        return total_loss


def create_optimized_training_config():
    """Create GPU-optimized training configuration."""
    return {
        'batch_size': 32,  # Adjust based on GPU memory
        'learning_rate': 1e-4,
        'warmup_steps': 4000,
        'max_epochs': 1000,
        'save_interval': 10,
        'log_interval': 100,
        'gradient_clip_norm': 1.0,
        'use_mixed_precision': True,
        'optimizer': 'adamw',
        'weight_decay': 1e-4,
    }


def benchmark_gpu_performance(model, batch_size=8, seq_len=100):
    """Benchmark model performance on GPU vs CPU."""
    print("üèÉ‚Äç‚ôÇÔ∏è Running performance benchmark...")
    
    dummy_input = tf.random.uniform((batch_size, seq_len), 0, 1000, dtype=tf.int32)
    
    # GPU benchmark
    if tf.config.list_physical_devices('GPU'):
        with tf.device('/GPU:0'):
            start_time = tf.timestamp()
            for _ in range(10):
                _ = model(dummy_input, training=False)
            gpu_time = tf.timestamp() - start_time
            print(f"‚ö° GPU inference time (10 runs): {gpu_time:.4f} seconds")
    
    # CPU benchmark
    with tf.device('/CPU:0'):
        start_time = tf.timestamp()
        for _ in range(10):
            _ = model(dummy_input, training=False)
        cpu_time = tf.timestamp() - start_time
        print(f"üêå CPU inference time (10 runs): {cpu_time:.4f} seconds")
    
    if tf.config.list_physical_devices('GPU'):
        speedup = cpu_time / gpu_time
        print(f"üöÄ GPU speedup: {speedup:.2f}x")


if __name__ == "__main__":
    print("üéµ Transformer TTS GPU-Optimized Model Test")
    print("=" * 50)
    
    # Test the model with GPU optimization
    config = create_transformer_tts_config()
    model = build_transformer_tts_model(config=config, use_gpu=True)
    
    # Test with dummy input
    dummy_text = tf.random.uniform((2, 20), 0, config['vocab_size'], dtype=tf.int32)
    
    print("\nüß™ Testing model inference...")
    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):
        output = model(dummy_text, training=False)
    
    print("\nüìã Model output shapes:")
    for key, value in output.items():
        print(f"  {key}: {value.shape}")
    
    # Run performance benchmark
    print("\n" + "=" * 50)
    benchmark_gpu_performance(model)
    
    print("\n‚úÖ GPU optimization test completed successfully!") 
